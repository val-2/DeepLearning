{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# PikaPikaGen: Text-to-Image Pokemon Sprite Generation with GAN\n",
        "\n",
        "This notebook implements a Generative Adversarial Network (GAN) for generating Pokemon sprites from textual descriptions, based on the encoder-decoder architecture with attention mechanism described in the instructions.\n",
        "\n",
        "## Architecture Overview:\n",
        "- **Text Encoder**: Transformer-based encoder with BERT-mini embeddings\n",
        "- **Generator**: CNN decoder with attention mechanism (from instructions)\n",
        "- **Discriminator**: CNN discriminator for adversarial training\n",
        "- **Attention Mechanism**: Links text features with image generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "#!pip install torch torchvision transformers pandas pillow requests matplotlib tqdm ipywidgets gradio\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset download utility\n",
        "def reporthook(block_num, block_size, total_size):\n",
        "    if block_num % 16384 == 0:\n",
        "        print(f\"Downloading... {block_num * block_size / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "def download_dataset_if_not_exists():\n",
        "    dataset_dir = \"dataset\"\n",
        "    pokedex_main_dir = os.path.join(dataset_dir, \"pokedex-main\")\n",
        "    zip_url = \"https://github.com/cristobalmitchell/pokedex/archive/refs/heads/main.zip\"\n",
        "    zip_path = \"pokedex_main.zip\"\n",
        "\n",
        "    # Check if dataset/pokedex-main already exists\n",
        "    if os.path.exists(pokedex_main_dir):\n",
        "        print(f\"{pokedex_main_dir} already exists. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    # Create dataset directory if it doesn't exist\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    # Download the zip file\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(zip_url, zip_path, reporthook)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "    # Extract the zip file into the dataset directory\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_dir)\n",
        "    print(\"Extraction complete.\")\n",
        "\n",
        "    # Optionally, remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "\n",
        "# Download the dataset\n",
        "download_dataset_if_not_exists()\n",
        "print(\"Dataset ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "\n",
        "class AugmentationPipeline:\n",
        "    def __init__(self, p=0.8):\n",
        "        self.p = p\n",
        "        self.transforms = T.RandomApply([\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "            # Applica trasformazioni affini (rotazione/scala) solo il 50% delle volte.\n",
        "            # Ho ridotto leggermente l'intensitÃ  (degrees=10).\n",
        "            T.RandomApply([\n",
        "                T.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05), fill=1)\n",
        "            ], p=0.5),\n",
        "\n",
        "            # Applica ColorJitter solo il 50% delle volte.\n",
        "            # I parametri sono giÃ  abbastanza bassi, quindi li manteniamo.\n",
        "            T.RandomApply([\n",
        "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n",
        "            ], p=0.5),\n",
        "\n",
        "            # --- Passo 4: RandomErasing (su Tensore) ---\n",
        "            # Ridotto la probabilitÃ  di applicazione.\n",
        "            # Ãˆ una tecnica forte, meglio usarla con parsimonia per iniziare.\n",
        "            T.RandomErasing(p=0.15, scale=(0.02, 0.1), ratio=(0.3, 3.3), value='random'),\n",
        "        ], p=self.p)\n",
        "\n",
        "    def apply(self, images):\n",
        "        return self.transforms(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Pokemon Dataset Class with modular augmentation support\n",
        "class PokemonDataset(Dataset):\n",
        "    def __init__(self, tokenizer, csv_path=\"dataset/pokedex-main/data/pokemon.csv\",\n",
        "                 image_dir=\"dataset/pokedex-main/images/small_images\",\n",
        "                 max_length=128, augmentation_pipeline=None):\n",
        "        \"\"\"\n",
        "        Dataset per Pokemon: testo (descrizione) -> immagine (sprite)\n",
        "        Enhanced with modular augmentation pipeline support\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(csv_path, encoding='utf-16 LE', delimiter='\\t')\n",
        "        self.image_dir = Path(image_dir)\n",
        "        print(f\"Dataset caricato: {len(self.df)} Pokemon con descrizioni e immagini\")\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.augmentation_pipeline = augmentation_pipeline\n",
        "\n",
        "        if self.augmentation_pipeline is not None:\n",
        "            self.final_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Resize((128, 128), antialias=True),\n",
        "                self.augmentation_pipeline,\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalizza a [-1, 1]\n",
        "            ])\n",
        "        else:\n",
        "            self.final_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Resize((128, 128), antialias=True),\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalizza a [-1, 1]\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Restituisce il numero totale di campioni\"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Restituisce un singolo campione: (testo_tokenizzato, immagine_tensor)\n",
        "        Full implementation matching pokemon_dataset.py\n",
        "        \"\"\"\n",
        "        # Ottieni la riga corrispondente\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # === PREPROCESSING DEL TESTO ===\n",
        "        description = str(row['description'])\n",
        "\n",
        "        # Tokenizza il testo\n",
        "        encoded = self.tokenizer(\n",
        "            description,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Estrai token_ids e attention_mask\n",
        "        text_ids = encoded['input_ids'].squeeze(0)  # Rimuovi la dimensione batch\n",
        "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
        "\n",
        "        # === CARICAMENTO E PREPROCESSING DELL'IMMAGINE ===\n",
        "        # Costruisce il percorso dell'immagine\n",
        "        image_filename = f\"{row['national_number']:03d}.png\"\n",
        "        image_path = self.image_dir/image_filename\n",
        "\n",
        "        # Carica l'immagine\n",
        "        image_rgba = Image.open(image_path).convert('RGBA')\n",
        "\n",
        "        # Gestisce la trasparenza: ricombina l'immagine con uno sfondo bianco\n",
        "        background = Image.new('RGB', image_rgba.size, (255, 255, 255))\n",
        "        background.paste(image_rgba, mask=image_rgba.split()[-1])\n",
        "\n",
        "        # Applica le trasformazioni finali (ToTensor, Resize, Normalize)\n",
        "        image_tensor = self.final_transform(background)\n",
        "\n",
        "        # Costruisce il risultato (matches pokemon_dataset.py structure)\n",
        "        sample = {\n",
        "            'text': text_ids,\n",
        "            'image': image_tensor,\n",
        "            'description': description,  # Per debug o visualizzazione\n",
        "            'pokemon_name': row['english_name'],\n",
        "            'idx': idx,\n",
        "            'attention_mask': attention_mask,\n",
        "        }\n",
        "\n",
        "        return sample\n",
        "\n",
        "def create_training_setup(tokenizer, train_val_split, batch_size, num_workers=0,\n",
        "                         num_viz_samples=4, random_seed=42, train_augmentation_pipeline=None):\n",
        "    \"\"\"\n",
        "    Crea un setup completo per il training con dataset, dataloader e batch fissi per visualizzazione.\n",
        "    Enhanced with modular augmentation pipeline support\n",
        "    \"\"\"\n",
        "    from torch.utils.data import random_split, TensorDataset, Subset\n",
        "\n",
        "    # --- Creazione dei Dataset ---\n",
        "    # Crea un'istanza per il training (con augmentazione) e la validazione (senza augmentazione)\n",
        "    train_full_dataset = PokemonDataset(tokenizer=tokenizer, augmentation_pipeline=train_augmentation_pipeline)\n",
        "    val_full_dataset = PokemonDataset(tokenizer=tokenizer, augmentation_pipeline=None)  # No augmentation for validation\n",
        "\n",
        "    # --- Divisione deterministica degli indici ---\n",
        "    assert len(train_full_dataset) == len(val_full_dataset)\n",
        "    dataset_size = len(train_full_dataset)\n",
        "    train_size = int(train_val_split * dataset_size)\n",
        "    val_size = dataset_size - train_size\n",
        "\n",
        "    train_indices_subset, val_indices_subset = random_split(\n",
        "        TensorDataset(torch.arange(dataset_size)),\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(random_seed),\n",
        "    )\n",
        "\n",
        "    train_dataset = Subset(train_full_dataset, train_indices_subset.indices)\n",
        "    val_dataset = Subset(val_full_dataset, val_indices_subset.indices)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # --- Creazione deterministica dei batch per la visualizzazione ---\n",
        "    vis_generator = torch.Generator().manual_seed(random_seed)\n",
        "    fixed_train_batch = next(\n",
        "        iter(\n",
        "            DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=num_viz_samples,\n",
        "                shuffle=True,\n",
        "                generator=vis_generator,\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    fixed_val_batch = next(\n",
        "        iter(DataLoader(val_dataset, batch_size=num_viz_samples, shuffle=False))\n",
        "    )  # la validazione non ha shuffle\n",
        "\n",
        "    vis_generator.manual_seed(random_seed)  # Reset per coerenza\n",
        "    fixed_train_attention_batch = next(\n",
        "        iter(\n",
        "            DataLoader(\n",
        "                train_dataset, batch_size=1, shuffle=True, generator=vis_generator\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    fixed_val_attention_batch = next(\n",
        "        iter(DataLoader(val_dataset, batch_size=1, shuffle=False))\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'train_loader': train_loader,\n",
        "        'val_loader': val_loader,\n",
        "        'fixed_train_batch': fixed_train_batch,\n",
        "        'fixed_val_batch': fixed_val_batch,\n",
        "        'fixed_train_attention_batch': fixed_train_attention_batch,\n",
        "        'fixed_val_attention_batch': fixed_val_attention_batch,\n",
        "        'train_dataset': train_dataset,\n",
        "        'val_dataset': val_dataset,\n",
        "    }\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-mini')\n",
        "\n",
        "# train_augmentation_pipeline = AugmentationPipeline()\n",
        "# Create the complete training setup using the function from pokemon_dataset.py\n",
        "print(\"Creating training setup with train/val split and fixed batches...\")\n",
        "training_setup = create_training_setup(\n",
        "    tokenizer=tokenizer,\n",
        "    train_val_split=0.9,\n",
        "    batch_size=8,\n",
        "    num_workers=0,\n",
        "    num_viz_samples=4,\n",
        "    random_seed=42,\n",
        "    train_augmentation_pipeline=None\n",
        ")\n",
        "\n",
        "# Extract components\n",
        "train_loader = training_setup['train_loader']\n",
        "val_loader = training_setup['val_loader']\n",
        "fixed_train_batch = training_setup['fixed_train_batch']\n",
        "fixed_val_batch = training_setup['fixed_val_batch']\n",
        "fixed_train_attention_batch = training_setup['fixed_train_attention_batch']\n",
        "fixed_val_attention_batch = training_setup['fixed_val_attention_batch']\n",
        "train_dataset = training_setup['train_dataset']\n",
        "val_dataset = training_setup['val_dataset']\n",
        "\n",
        "print(f\"Training setup complete!\")\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Train loader batches: {len(train_loader)}\")\n",
        "print(f\"Val loader batches: {len(val_loader)}\")\n",
        "\n",
        "# Test the training setup with fixed batches\n",
        "print(f\"\\nFixed batch shapes:\")\n",
        "print(f\"  Train batch - Images: {fixed_train_batch['image'].shape}\")\n",
        "print(f\"  Train batch - Text: {fixed_train_batch['text'].shape}\")\n",
        "print(f\"  Train batch - Attention: {fixed_train_batch['attention_mask'].shape}\")\n",
        "print(f\"  Val batch - Images: {fixed_val_batch['image'].shape}\")\n",
        "\n",
        "# Display sample images from fixed batches\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "for i in range(4):\n",
        "    # Fixed train batch images\n",
        "    img = (fixed_train_batch['image'][i] + 1) / 2.0  # Denormalize\n",
        "    axes[0, i].imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
        "    axes[0, i].set_title(f\"Train: {fixed_train_batch['pokemon_name'][i]}\")\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    # Fixed val batch images\n",
        "    img = (fixed_val_batch['image'][i] + 1) / 2.0  # Denormalize\n",
        "    axes[1, i].imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
        "    axes[1, i].set_title(f\"Val: {fixed_val_batch['pokemon_name'][i]}\")\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.suptitle(\"Fixed Batches for Training Visualization\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Set the main dataloader to use train_loader for consistency\n",
        "dataloader = train_loader\n",
        "sample_batch = fixed_train_batch\n",
        "\n",
        "print(f\"\\nâœ… Dataset and batches loaded successfully from pokemon_dataset.py functionality!\")\n",
        "print(f\"Ready for training with proper train/val split and fixed visualization batches.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Demonstrate that augmentations are working and different for the same Pokemon\n",
        "print(f\"\\nðŸ”„ AUGMENTATION VERIFICATION:\")\n",
        "print(\"Testing that augmentations produce different results for the same Pokemon...\")\n",
        "\n",
        "train_augmentation_pipeline = AugmentationPipeline()\n",
        "\n",
        "# Pick the first Pokemon from the training dataset and show it with different augmentations\n",
        "test_pokemon_idx = 0\n",
        "original_dataset = PokemonDataset(tokenizer=tokenizer, augmentation_pipeline=None)  # No augmentation\n",
        "augmented_dataset = PokemonDataset(tokenizer=tokenizer, augmentation_pipeline=train_augmentation_pipeline.transforms)  # With augmentation\n",
        "\n",
        "# Get the same Pokemon multiple times to see different augmentations\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "fig.suptitle(\"Augmentation Verification: Same Pokemon with Different Augmentations\", fontsize=16)\n",
        "\n",
        "# Row 0: Original (no augmentation) - same image repeated\n",
        "original_sample = original_dataset[test_pokemon_idx]\n",
        "for i in range(5):\n",
        "    img = (original_sample['image'] + 1) / 2.0  # Denormalize\n",
        "    axes[0, i].imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
        "    axes[0, i].set_title(f\"Original {i+1}\")\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "# Row 1: Augmented (should be different each time)\n",
        "for i in range(5):\n",
        "    augmented_sample = augmented_dataset[test_pokemon_idx]  # Same index, different augmentation\n",
        "    img = (augmented_sample['image'] + 1) / 2.0  # Denormalize\n",
        "    axes[1, i].imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
        "    axes[1, i].set_title(f\"Augmented {i+1}\")\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "# Add row labels\n",
        "axes[0, 0].text(-0.1, 0.5, 'No Augmentation\\n(Should be identical)',\n",
        "                ha='center', va='center', rotation='vertical',\n",
        "                fontsize=12, transform=axes[0, 0].transAxes)\n",
        "axes[1, 0].text(-0.1, 0.5, 'With Augmentation\\n(Should be different)',\n",
        "                ha='center', va='center', rotation='vertical',\n",
        "                fontsize=12, transform=axes[1, 0].transAxes)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Pokemon tested: {original_sample['pokemon_name']} (#{original_sample['idx']})\")\n",
        "print(f\"Description: {original_sample['description'][:60]}...\")\n",
        "print(f\"âœ… If augmentations are working correctly, the bottom row should show different variations!\")\n",
        "print(f\"   - Look for differences in: rotation, translation, color, brightness, horizontal flip\")\n",
        "print(f\"   - The top row should be identical (no augmentation)\")\n",
        "print(f\"   - This proves augmentations will be different at each epoch for the same Pokemon!\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Model Architecture Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder per processare il testo.\n",
        "    Usa gli embedding di bert-mini e li fa passare in un Transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=\"prajjwal1/bert-mini\", fine_tune_embeddings=True):\n",
        "        super().__init__()\n",
        "        # Carica il modello bert-mini pre-addestrato per estrarre gli embedding\n",
        "        bert_mini_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # Estrae lo strato di embedding\n",
        "        self.embedding = bert_mini_model.embeddings\n",
        "\n",
        "        # Imposta se fare il fine-tuning degli embedding durante il training\n",
        "        for param in self.embedding.parameters():\n",
        "            param.requires_grad = fine_tune_embeddings\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=256, nhead=4, dim_feedforward=1024, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "\n",
        "    def forward(self, token_ids, attention_mask=None):\n",
        "        # 1. Ottieni gli embedding dai token ID\n",
        "        # Shape: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
        "        embedded_text = self.embedding(token_ids)\n",
        "\n",
        "        # 2. Prepara la maschera di padding per il TransformerEncoder\n",
        "        # La maschera di HuggingFace Ã¨ 1 per i token reali, 0 per il padding.\n",
        "        # TransformerEncoder si aspetta True per le posizioni da ignorare (padding).\n",
        "        src_key_padding_mask = None\n",
        "        if attention_mask is not None:\n",
        "            src_key_padding_mask = (attention_mask == 0)\n",
        "\n",
        "        # 3. Passa gli embedding attraverso il Transformer Encoder con la maschera\n",
        "        # Shape: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, embedding_dim)\n",
        "        encoder_output = self.transformer_encoder(\n",
        "            src=embedded_text,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "        return encoder_output\n",
        "\n",
        "\n",
        "class ImageCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Modulo di Cross-Attention.\n",
        "    Permette a una sequenza di query (dall'immagine) di \"prestare attenzione\"\n",
        "    a una sequenza di key/value (dal testo), gestendo internamente\n",
        "    il reshaping dei tensori e la maschera di padding.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim, num_heads=num_heads, batch_first=True\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, image_features, text_features, key_padding_mask=None):\n",
        "        # query: (B, C, H, W) - Feature dell'immagine (spaziale)\n",
        "        # key/value: (B, seq_len, embed_dim) - Output dell'encoder di testo\n",
        "        # key_padding_mask: (B, seq_len) - Maschera dal tokenizer\n",
        "\n",
        "        B, C, H, W = image_features.shape\n",
        "\n",
        "        # 1. Prepara la query (feature dell'immagine)\n",
        "        # Reshape da spaziale a sequenza: (B, C, H, W) -> (B, H*W, C)\n",
        "        query_seq = image_features.view(B, C, H * W).permute(0, 2, 1)\n",
        "        query_norm = self.layer_norm(query_seq)\n",
        "\n",
        "        # 2. Prepara la maschera di padding per l'attenzione\n",
        "        # La maschera di HuggingFace Ã¨ 1 per i token reali, 0 per il padding.\n",
        "        # MultiheadAttention si aspetta True per le posizioni da ignorare.\n",
        "        if key_padding_mask is not None:\n",
        "            mask = (key_padding_mask == 0)\n",
        "        else:\n",
        "            mask = None\n",
        "\n",
        "        # 3. Applica l'attenzione\n",
        "        attn_output, attn_weights = self.attention(\n",
        "            query=query_norm,\n",
        "            key=text_features,\n",
        "            value=text_features,\n",
        "            key_padding_mask=mask,\n",
        "            need_weights=True\n",
        "        )\n",
        "        # attn_output: (B, H*W, C)\n",
        "\n",
        "        # 4. Riconverti l'output nella forma spaziale originale\n",
        "        # (B, H*W, C) -> (B, C, H*W) -> (B, C, H, W)\n",
        "        attn_output_spatial = attn_output.permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "        return attn_output_spatial, attn_weights\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Blocco del Generatore come da istruzioni:\n",
        "    Attenzione (opzionale) -> Fusione -> Upsampling (ConvTranspose) -> Normalizzazione -> Attivazione.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, use_attention=True, text_embed_dim=256, nhead=4):\n",
        "        super().__init__()\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        if self.use_attention:\n",
        "            if in_channels != text_embed_dim:\n",
        "                raise ValueError(\"in_channels must be equal to text_embedding_dim for attention.\")\n",
        "            self.cross_attention = ImageCrossAttention(embed_dim=in_channels, num_heads=nhead)\n",
        "            # Nuova convolution per fondere le feature dell'immagine con il contesto del testo\n",
        "            self.fusion_conv = nn.Conv2d(in_channels * 2, in_channels, kernel_size=1, bias=False)\n",
        "\n",
        "        # Blocco di upsampling come da istruzioni\n",
        "        self.upsample_block = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.GroupNorm(1, out_channels), # Equivalente a LayerNorm per feature map (N, C, H, W)\n",
        "            nn.LeakyReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, encoder_output=None, attention_mask=None):\n",
        "        attn_weights = None\n",
        "        if self.use_attention:\n",
        "            if encoder_output is None or attention_mask is None:\n",
        "                raise ValueError(\"encoder_output and attention_mask must be provided for attention.\")\n",
        "\n",
        "            attn_output, attn_weights = self.cross_attention(\n",
        "                image_features=x,\n",
        "                text_features=encoder_output,\n",
        "                key_padding_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            # Concatena le feature originali (x) con il contesto (attn_output)\n",
        "            # e le fonde con una convoluzione 1x1.\n",
        "            fused_features = torch.cat([x, attn_output], dim=1) # Shape: (B, 2*C, H, W)\n",
        "            x = self.fusion_conv(fused_features) # Shape: (B, C, H, W)\n",
        "\n",
        "        # Apply the U-Net style sequence\n",
        "        x = self.upsample_block(x)\n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "class ImageDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder CNN (Generatore) che sintetizza l'immagine.\n",
        "    Questa versione usa l'attenzione per-step fin dall'inizio.\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_dim, text_embed_dim, final_image_channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Meccanismo per calcolare i punteggi di attenzione per il contesto iniziale.\n",
        "        self.initial_context_scorer = nn.Sequential(\n",
        "            nn.Linear(in_features=text_embed_dim, out_features=512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=512, out_features=1)\n",
        "            # Il Softmax viene applicato nel forward pass per poter usare la maschera\n",
        "        )\n",
        "\n",
        "        # Proiezione lineare iniziale a una feature map 4x4.\n",
        "        self.initial_projection = nn.Sequential(\n",
        "            nn.Linear(noise_dim + text_embed_dim, 256 * 4 * 4),\n",
        "            nn.GroupNorm(1, 256 * 4 * 4),\n",
        "            nn.LeakyReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Blocchi del decoder basati su GeneratorBlock\n",
        "        self.blocks = nn.ModuleList([\n",
        "            # Input: (B, 256, 4, 4)   -> Output: (B, 256, 8, 8)\n",
        "            DecoderBlock(in_channels=256, out_channels=256, use_attention=True),\n",
        "            # Input: (B, 256, 8, 8)   -> Output: (B, 256, 16, 16)\n",
        "            DecoderBlock(in_channels=256, out_channels=256, use_attention=True),\n",
        "            # Input: (B, 256, 16, 16)  -> Output: (B, 128, 32, 32)\n",
        "            DecoderBlock(in_channels=256, out_channels=128, use_attention=True),\n",
        "            # Input: (B, 128, 32, 32)  -> Output: (B, 64, 64, 64)\n",
        "            DecoderBlock(in_channels=128, out_channels=64, use_attention=False),\n",
        "            # Input: (B, 64, 64, 64)  -> Output: (B, 32, 128, 128)\n",
        "            DecoderBlock(in_channels=64, out_channels=32, use_attention=False),\n",
        "            # # Input: (B, 32, 128, 128) -> Output: (B, 16, 256, 256)\n",
        "            # DecoderBlock(in_channels=32, out_channels=16, use_attention=False),\n",
        "        ])\n",
        "\n",
        "        # Layer finale per portare ai canali RGB\n",
        "        # Input: (B, 16, 256, 256) -> Output: (B, 3, 256, 256)\n",
        "        self.final_conv = nn.Conv2d(32, final_image_channels, kernel_size=3, padding=1)\n",
        "        self.final_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, noise, encoder_output_full, attention_mask):\n",
        "        # noise.shape: (B, noise_dim)\n",
        "        # encoder_output_full.shape: (B, seq_len, text_embed_dim)\n",
        "        # attention_mask.shape: (B, seq_len)\n",
        "\n",
        "        # 1. Calcola il vettore di contesto iniziale con una media pesata (ATTENZIONE #1)\n",
        "        # Calcola i punteggi (logits) per ogni token del testo\n",
        "        attn_scores = self.initial_context_scorer(encoder_output_full)\n",
        "\n",
        "        # Applica la maschera di padding prima del softmax.\n",
        "        # Imposta i punteggi dei token di padding a -infinito.\n",
        "        if attention_mask is not None:\n",
        "            # La maschera Ã¨ (B, seq_len), i punteggi (B, seq_len, 1)\n",
        "            # Il broadcast si occupa di allineare le dimensioni.\n",
        "            attn_scores.masked_fill_(attention_mask.unsqueeze(-1) == 0, -1e9)\n",
        "\n",
        "        # Ora applica il softmax per ottenere i pesi.\n",
        "        # attention_weights.shape: (B, seq_len, 1)\n",
        "        attention_weights = torch.softmax(attn_scores, dim=1)\n",
        "\n",
        "        # Calcola il contesto come media pesata degli output dell'encoder.\n",
        "        # context_vector.shape: (B, text_embed_dim)\n",
        "        context_vector = torch.sum(attention_weights * encoder_output_full, dim=1)\n",
        "\n",
        "        # 2. Prepara il vettore di input iniziale per la proiezione\n",
        "        #    Si usa direttamente il rumore 'noise' invece del vettore di stile 'w'\n",
        "        # initial_input.shape: (B, noise_dim + text_embed_dim)\n",
        "        initial_input = torch.cat([noise, context_vector], dim=1)\n",
        "\n",
        "        # 3. Proietta e rimodella\n",
        "        # x.shape: (B, 256 * 4 * 4)\n",
        "        x = self.initial_projection(initial_input)\n",
        "        # x.shape: (B, 256, 4, 4)\n",
        "        x = x.view(x.size(0), 256, 4, 4)\n",
        "\n",
        "        # 5. Passa attraverso i blocchi del decoder\n",
        "        attention_maps = []\n",
        "        for block in self.blocks:\n",
        "             encoder_ctx = encoder_output_full if block.use_attention else None\n",
        "             mask_ctx = attention_mask if block.use_attention else None\n",
        "             # La shape di x viene upsamplata in ogni blocco (es. 4x4 -> 8x8)\n",
        "             x, attn_weights = block(x, encoder_ctx, mask_ctx)\n",
        "\n",
        "             if attn_weights is not None:\n",
        "                # attn_weights.shape: (B, H*W, seq_len)\n",
        "                attention_maps.append(attn_weights)\n",
        "\n",
        "        # 6. Layer finale\n",
        "        # x.shape: (B, 3, 256, 256)\n",
        "        x = self.final_conv(x)\n",
        "        # x.shape: (B, 3, 256, 256)\n",
        "        x = self.final_activation(x)\n",
        "        return x, attention_maps, attention_weights\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Modello completo che unisce Encoder e Decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, text_encoder_model_name=\"prajjwal1/bert-mini\", noise_dim=100):\n",
        "        super().__init__()\n",
        "        self.text_encoder = TextEncoder(\n",
        "            model_name=text_encoder_model_name,\n",
        "        )\n",
        "\n",
        "        text_embed_dim = 256\n",
        "\n",
        "        self.image_decoder = ImageDecoder(\n",
        "            noise_dim=noise_dim,\n",
        "            text_embed_dim=text_embed_dim\n",
        "        )\n",
        "\n",
        "        self.noise_dim = noise_dim\n",
        "\n",
        "    def forward(self, token_ids, attention_mask, return_attentions=False):\n",
        "        # token_ids.shape: (batch_size, seq_len)\n",
        "        # attention_mask.shape: (batch_size, seq_len)\n",
        "        # Genera rumore casuale per il batch\n",
        "        batch_size = token_ids.size(0)\n",
        "        # noise.shape: (batch_size, noise_dim)\n",
        "        noise = torch.randn(batch_size, self.noise_dim, device=token_ids.device)\n",
        "\n",
        "        # 1. Codifica il testo per ottenere i vettori di ogni parola\n",
        "        # encoder_output.shape: (batch_size, seq_len, text_embed_dim)\n",
        "        encoder_output = self.text_encoder(token_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # 2. Genera l'immagine usando l'output completo dell'encoder\n",
        "        #    Il decoder calcolerÃ  internamente sia il contesto iniziale (ATTENZIONE #1)\n",
        "        #    sia l'attenzione per-step (ATTENZIONE #2)\n",
        "        # generated_image.shape: (batch_size, 3, 256, 256)\n",
        "        generated_image, attention_maps, initial_attention_weights = self.image_decoder(noise, encoder_output, attention_mask)\n",
        "\n",
        "        if return_attentions:\n",
        "            return generated_image, attention_maps, initial_attention_weights\n",
        "        return generated_image\n",
        "\n",
        "\n",
        "\n",
        "# Test the generator\n",
        "generator = Generator().to(device)\n",
        "with torch.no_grad():\n",
        "    generated_images = generator(\n",
        "        sample_batch['text'][:2].to(device),\n",
        "        sample_batch['attention_mask'][:2].to(device)\n",
        "    )\n",
        "print(f\"Generator output shape: {generated_images.shape}\")\n",
        "\n",
        "# Show a sample generated image to verify it works\n",
        "plt.figure(figsize=(8, 4))\n",
        "for i in range(2):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    img = (generated_images[i].cpu() + 1) / 2.0  # Denormalize\n",
        "    plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
        "    plt.title(f\"Generated Sample {i+1}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"âœ… Generator test successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Discriminator for 256x256 images\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, text_dim=256, img_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.text_encoder = TextEncoder()\n",
        "\n",
        "        self.img_path = nn.Sequential(\n",
        "            # # 256x256 -> 128x128\n",
        "            # nn.Conv2d(img_channels, 16, 4, 2, 1, bias=False),\n",
        "            # nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # 128x128 -> 64x64\n",
        "            nn.Conv2d(img_channels, 32, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # 64x64 -> 32x32\n",
        "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # 32x32 -> 16x16\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # 16x16 -> 8x8\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # 8x8 -> 4x4\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        # Text encoder for discriminator\n",
        "        self.text_path = nn.Sequential(\n",
        "            nn.Linear(text_dim, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 512)\n",
        "        )\n",
        "\n",
        "        # Final classifier (updated for new feature size)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 4 * 4 + 512, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, images, text_features, text_mask):\n",
        "        # Scale images to 64x64 first\n",
        "        # images = self.resize(images)\n",
        "\n",
        "        # Encode image\n",
        "        img_features = self.img_path(images)\n",
        "        img_features = img_features.view(img_features.size(0), -1)  # Flatten\n",
        "\n",
        "        # Encode text (mean pooling)\n",
        "        global_full_text = self.text_encoder(text_features, text_mask)\n",
        "        global_text = global_full_text.mean(dim=1)\n",
        "        text_features_encoded = self.text_path(global_text)\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat([img_features, text_features_encoded], dim=1)\n",
        "\n",
        "        # Classify\n",
        "        output = self.classifier(combined)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Test the discriminator\n",
        "discriminator = Discriminator().to(device)\n",
        "with torch.no_grad():\n",
        "    # Generate test images first\n",
        "    test_generated_images = generator(\n",
        "        sample_batch['text'][:2].to(device),\n",
        "        sample_batch['attention_mask'][:2].to(device)\n",
        "    )\n",
        "\n",
        "    disc_output = discriminator(\n",
        "        test_generated_images,  # Using full 256x256 images\n",
        "        sample_batch['text'][:2].to(device),\n",
        "        sample_batch['attention_mask'][:2].to(device)\n",
        "    )\n",
        "print(f\"Discriminator output shape: {disc_output.shape}\")\n",
        "print(f\"Test generated images shape: {test_generated_images.shape}\")\n",
        "print(\"âœ… Discriminator now accepts 256x256 images directly!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Training Setup and Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from torchvision.models import VGG19_Weights\n",
        "\n",
        "\n",
        "class VGGPerceptualLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Perceptual loss using VGG19 pretrained on ImageNet.\n",
        "    We extract features at:\n",
        "      - relu1_2  (index: 3)\n",
        "      - relu2_2  (index: 8)\n",
        "      - relu3_2  (index: 17)\n",
        "      - relu4_2  (index: 26)\n",
        "    Then compute L1 distance between those feature maps.\n",
        "    Input images are in [-1,1]. We convert to [0,1], then normalize with ImageNet stats.\n",
        "    \"\"\"\n",
        "    def __init__(self, device):\n",
        "        super(VGGPerceptualLoss, self).__init__()\n",
        "        vgg19_features = models.vgg19(weights=VGG19_Weights.DEFAULT).features.to(device).eval()\n",
        "        # We only need layers up to 26 (relu4_2)\n",
        "        self.slices = nn.ModuleDict({\n",
        "            \"relu1_2\": nn.Sequential(*list(vgg19_features.children())[:4]),     # conv1_1, relu1_1, conv1_2, relu1_2\n",
        "            \"relu2_2\": nn.Sequential(*list(vgg19_features.children())[4:9]),    # pool1, conv2_1, relu2_1, conv2_2, relu2_2\n",
        "            \"relu3_2\": nn.Sequential(*list(vgg19_features.children())[9:18]),   # pool2, conv3_1, relu3_1, conv3_2, relu3_2, ...\n",
        "            \"relu4_2\": nn.Sequential(*list(vgg19_features.children())[18:27])   # pool3, conv4_1, relu4_1, conv4_2, relu4_2\n",
        "        })\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.l1 = nn.L1Loss()\n",
        "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, img_gen, img_ref):\n",
        "        \"\"\"\n",
        "        img_gen, img_ref: [B,3,H,W] in range [-1,1].\n",
        "        Return: sum of L1 distances between VGG feature maps at chosen layers.\n",
        "        \"\"\"\n",
        "        # Convert to [0,1]\n",
        "        gen = (img_gen + 1.0) / 2.0\n",
        "        ref = (img_ref + 1.0) / 2.0\n",
        "        # Normalize\n",
        "        gen_norm = (gen - self.mean) / self.std\n",
        "        ref_norm = (ref - self.mean) / self.std\n",
        "\n",
        "        loss = 0.0\n",
        "        x_gen = gen_norm\n",
        "        x_ref = ref_norm\n",
        "        for slice_mod in self.slices.values():\n",
        "            x_gen = slice_mod(x_gen)\n",
        "            x_ref = slice_mod(x_ref)\n",
        "            loss += self.l1(x_gen, x_ref)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class SobelLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes the Sobel loss between two images, which encourages edge similarity.\n",
        "    This loss operates on the grayscale versions of the input images.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(SobelLoss, self).__init__()\n",
        "        # Sobel kernels for edge detection\n",
        "        kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        self.register_buffer(\"kernel_x\", kernel_x)\n",
        "        self.register_buffer(\"kernel_y\", kernel_y)\n",
        "        self.l1 = nn.L1Loss()\n",
        "\n",
        "        # Grayscale conversion weights (ITU-R BT.601)\n",
        "        self.register_buffer(\"rgb_to_gray_weights\", torch.tensor([0.299, 0.587, 0.114]).view(1, 3, 1, 1))\n",
        "\n",
        "    def _get_edges(self, img):\n",
        "        \"\"\"\n",
        "        Converts an RGB image to grayscale and applies Sobel filters.\n",
        "        Args:\n",
        "            img: [B, 3, H, W] image tensor in range [-1, 1].\n",
        "        Returns:\n",
        "            Gradient magnitude map [B, 1, H, W].\n",
        "        \"\"\"\n",
        "        # Ensure input is 4D\n",
        "        if img.dim() != 4:\n",
        "            raise ValueError(f\"Expected 4D input (got {img.dim()}D)\")\n",
        "\n",
        "        # Convert from [-1, 1] to [0, 1]\n",
        "        img = (img + 1.0) / 2.0\n",
        "\n",
        "        # Convert to grayscale\n",
        "        # The weights need to be on the same device as the image.\n",
        "        grayscale_img = F.conv2d(img, self.rgb_to_gray_weights.to(img.device)) # type: ignore\n",
        "\n",
        "        # Apply Sobel filters. Kernels also need to be on the correct device.\n",
        "        grad_x = F.conv2d(grayscale_img, self.kernel_x.to(img.device), padding=1) # type: ignore\n",
        "        grad_y = F.conv2d(grayscale_img, self.kernel_y.to(img.device), padding=1) # type: ignore\n",
        "\n",
        "        # Compute gradient magnitude\n",
        "        edges = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6) # add epsilon for stability\n",
        "        return edges\n",
        "\n",
        "    def forward(self, img_gen, img_ref):\n",
        "        \"\"\"\n",
        "        img_gen, img_ref: [B, 3, H, W] in range [-1, 1].\n",
        "        Returns: L1 loss between the edge maps of the two images.\n",
        "        \"\"\"\n",
        "        edges_gen = self._get_edges(img_gen)\n",
        "        edges_ref = self._get_edges(img_ref)\n",
        "        return self.l1(edges_gen, edges_ref)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Training utilities and visualization functions from utils.py\n",
        "def weights_init(m):\n",
        "    \"\"\"Initialize model weights\"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "def denormalize_image(tensor):\n",
        "    \"\"\"\n",
        "    Denormalizza un tensore immagine dall'intervallo [-1, 1] a [0, 1] per la visualizzazione.\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Il tensore dell'immagine, con valori in [-1, 1].\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Il tensore denormalizzato con valori in [0, 1].\n",
        "    \"\"\"\n",
        "    tensor = (tensor + 1) / 2\n",
        "    return tensor.clamp(0, 1)\n",
        "\n",
        "def save_plot_losses(losses_g, losses_d, losses_recon=None, output_dir=\"training_output\", show_inline=True):\n",
        "    \"\"\"\n",
        "    Genera e salva un plot delle loss del generatore e del discriminatore.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.plot(losses_g, label=\"Generator Loss\", color=\"blue\")\n",
        "    ax.plot(losses_d, label=\"Discriminator Loss\", color=\"red\")\n",
        "    if losses_recon is not None:\n",
        "        ax.plot(losses_recon, label=\"Reconstruction Loss\", color=\"green\")\n",
        "    ax.set_title(\"Training Losses\")\n",
        "    ax.set_xlabel(\"Epochs\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "    save_path = os.path.join(output_dir, \"training_losses.png\")\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"Grafico delle loss salvato in: {save_path}\")\n",
        "\n",
        "    if show_inline:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)\n",
        "\n",
        "def save_plot_non_gan_losses(train_losses_history, val_losses_history, output_dir=\"training_output\", show_inline=True, filter_losses=None):\n",
        "    \"\"\"\n",
        "    Generates and saves plots of losses for non-GAN models with multiple loss components.\n",
        "\n",
        "    Args:\n",
        "        train_losses_history: List of dicts containing training losses per epoch\n",
        "                             e.g., [{'l1': 0.5, 'sobel': 0.3, 'ssim': 0.2}, ...]\n",
        "        val_losses_history: List of dicts containing validation losses per epoch\n",
        "        output_dir: Directory to save the plot\n",
        "        show_inline: Whether to display the plot inline\n",
        "        filter_losses: Optional list of loss names to plot. If None, plots all losses.\n",
        "                      e.g., ['l1', 'sobel'] to only plot those specific losses\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if not train_losses_history or not val_losses_history:\n",
        "        print(\"No loss history to plot\")\n",
        "        return\n",
        "\n",
        "    # Extract all unique loss keys from both training and validation\n",
        "    all_keys = set()\n",
        "    for losses_dict in train_losses_history + val_losses_history:\n",
        "        all_keys.update(losses_dict.keys())\n",
        "\n",
        "    # Filter out non-numeric keys if any\n",
        "    loss_keys = [key for key in all_keys if key not in ['epoch']]\n",
        "\n",
        "    # Apply filter if specified\n",
        "    if filter_losses is not None:\n",
        "        loss_keys = [key for key in loss_keys if key in filter_losses]\n",
        "\n",
        "    loss_keys = sorted(loss_keys)  # Sort for consistent ordering\n",
        "\n",
        "    if not loss_keys:\n",
        "        print(\"No valid loss keys found\")\n",
        "        return\n",
        "\n",
        "    # Create subplots\n",
        "    n_losses = len(loss_keys)\n",
        "    cols = min(3, n_losses)  # Max 3 columns\n",
        "    rows = (n_losses + cols - 1) // cols  # Ceiling division\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows))\n",
        "    if n_losses == 1:\n",
        "        axes = [axes]\n",
        "    elif rows == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    fig.suptitle(\"Training and Validation Losses\", fontsize=16, y=0.98)\n",
        "\n",
        "    for i, loss_key in enumerate(loss_keys):\n",
        "        row = i // cols\n",
        "        col = i % cols\n",
        "        ax = axes[row, col] if rows > 1 else axes[col]\n",
        "\n",
        "        # Extract train and validation losses for this key\n",
        "        train_values = [losses.get(loss_key, 0) for losses in train_losses_history]\n",
        "        val_values = [losses.get(loss_key, 0) for losses in val_losses_history]\n",
        "\n",
        "        epochs_train = range(1, len(train_values) + 1)\n",
        "        epochs_val = range(1, len(val_values) + 1)\n",
        "\n",
        "        # Plot training and validation curves\n",
        "        if train_values:\n",
        "            ax.plot(epochs_train, train_values, label=f\"Train {loss_key}\", color=\"blue\", linewidth=1.5)\n",
        "        if val_values:\n",
        "            ax.plot(epochs_val, val_values, label=f\"Val {loss_key}\", color=\"red\", linewidth=1.5, linestyle='--')\n",
        "\n",
        "        ax.set_title(f\"{loss_key.capitalize()} Loss\", fontsize=12)\n",
        "        ax.set_xlabel(\"Epoch\")\n",
        "        ax.set_ylabel(\"Loss\")\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Set y-axis to start from 0 for better visualization\n",
        "        ax.set_ylim(bottom=0)\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for i in range(n_losses, rows * cols):\n",
        "        row = i // cols\n",
        "        col = i % cols\n",
        "        if rows > 1:\n",
        "            axes[row, col].set_visible(False)\n",
        "        else:\n",
        "            axes[col].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot\n",
        "    save_path = os.path.join(output_dir, \"non_gan_training_losses.png\")\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"Non-GAN training losses plot saved to: {save_path}\")\n",
        "\n",
        "    if show_inline:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "def save_comparison_grid(epoch, model, batch, set_name, device, output_dir=\"training_output\", show_inline=True):\n",
        "    \"\"\"\n",
        "    Genera e salva/mostra una griglia di confronto orizzontale (reale vs. generato).\n",
        "    Enhanced version from utils.py\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "    token_ids = batch[\"text\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    real_images = batch[\"image\"]\n",
        "    pokemon_ids = batch[\"idx\"]\n",
        "    descriptions = batch[\"description\"]\n",
        "    num_images = real_images.size(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_images = model(token_ids, attention_mask)\n",
        "\n",
        "    fig, axs = plt.subplots(2, num_images, figsize=(4 * num_images, 8.5))\n",
        "    fig.suptitle(\n",
        "        f\"Epoch {epoch} - {set_name.capitalize()} Comparison\", fontsize=16, y=0.98\n",
        "    )\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Riga 0: Immagini Reali\n",
        "        ax_real = axs[0, i]\n",
        "        ax_real.imshow(denormalize_image(real_images[i].cpu()).permute(1, 2, 0))\n",
        "        ax_real.set_title(f\"#{pokemon_ids[i]}: {descriptions[i][:35]}...\", fontsize=10)\n",
        "        ax_real.axis(\"off\")\n",
        "\n",
        "        # Riga 1: Immagini Generate\n",
        "        ax_gen = axs[1, i]\n",
        "        ax_gen.imshow(denormalize_image(generated_images[i].cpu()).permute(1, 2, 0))\n",
        "        ax_gen.axis(\"off\")\n",
        "\n",
        "    axs[0, 0].text(\n",
        "        -0.1,\n",
        "        0.5,\n",
        "        \"Real\",\n",
        "        ha=\"center\",\n",
        "        va=\"center\",\n",
        "        rotation=\"vertical\",\n",
        "        fontsize=14,\n",
        "        transform=axs[0, 0].transAxes,\n",
        "    )\n",
        "    axs[1, 0].text(\n",
        "        -0.1,\n",
        "        0.5,\n",
        "        \"Generated\",\n",
        "        ha=\"center\",\n",
        "        va=\"center\",\n",
        "        rotation=\"vertical\",\n",
        "        fontsize=14,\n",
        "        transform=axs[1, 0].transAxes,\n",
        "    )\n",
        "\n",
        "    plt.tight_layout(rect=(0, 0, 1, 0.95))\n",
        "\n",
        "    # Salva sempre l'immagine\n",
        "    save_path = os.path.join(output_dir, f\"{epoch:03d}_{set_name}_comparison.png\")\n",
        "    plt.savefig(save_path)\n",
        "\n",
        "    if show_inline:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)\n",
        "\n",
        "def save_attention_visualization(epoch, model, tokenizer, batch, device, set_name, output_dir=\"training_output\", show_inline=True):\n",
        "    \"\"\"\n",
        "    Genera e salva una visualizzazione dell'attenzione multi-livello in stile griglia.\n",
        "    Enhanced version from utils.py\n",
        "\n",
        "    L'immagine mostra:\n",
        "    1. In alto, l'immagine generata e il prompt.\n",
        "    2. Sotto, un bar chart dell'attenzione iniziale (contesto globale).\n",
        "    3. Di seguito, una serie di griglie, una per ogni strato di attenzione del decoder.\n",
        "       Ciascuna griglia mostra le mappe di calore pure per ogni token rilevante.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        token_ids = batch[\"text\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        if token_ids.dim() > 1:  # Assicura un batch di 1\n",
        "            token_ids = token_ids[0].unsqueeze(0)\n",
        "            attention_mask = attention_mask[0].unsqueeze(0)\n",
        "\n",
        "        pokemon_id = batch[\"idx\"][0]\n",
        "        description = batch[\"description\"][0]\n",
        "\n",
        "        model_to_use = model.module if isinstance(model, nn.DataParallel) else model\n",
        "        generated_image, attention_maps, initial_context_weights = model_to_use(\n",
        "            token_ids, attention_mask, return_attentions=True\n",
        "        )\n",
        "\n",
        "    decoder_attention_maps = [m for m in attention_maps if m is not None]\n",
        "\n",
        "    if not decoder_attention_maps or initial_context_weights is None:\n",
        "        print(\n",
        "            f\"Epoch {epoch}: Mappe di attenzione non disponibili. Salto la visualizzazione.\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    tokens_all = tokenizer.convert_ids_to_tokens(token_ids.squeeze(0))\n",
        "    display_tokens = []\n",
        "    for i, token in enumerate(tokens_all):\n",
        "        if (\n",
        "            token not in [tokenizer.sep_token, tokenizer.pad_token]\n",
        "            and attention_mask[0, i] == 1\n",
        "        ):\n",
        "            display_tokens.append({\"token\": token, \"index\": i})\n",
        "\n",
        "    if not display_tokens:\n",
        "        print(\n",
        "            f\"Epoch {epoch}: Nessun token valido da visualizzare per '{description}'. Salto.\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    token_indices_to_display = [t[\"index\"] for t in display_tokens]\n",
        "    img_tensor_cpu = denormalize_image(generated_image.squeeze(0).cpu()).permute(\n",
        "        1, 2, 0\n",
        "    )\n",
        "    num_decoder_layers = len(decoder_attention_maps)\n",
        "    num_tokens = len(display_tokens)\n",
        "\n",
        "    # --- Creazione del Plot ---\n",
        "    # Calcola dinamicamente layout e dimensioni\n",
        "    cols = min(num_tokens, 8)\n",
        "    rows_per_layer = (num_tokens + cols - 1) // cols\n",
        "    num_main_rows = (\n",
        "        2 + num_decoder_layers\n",
        "    )  # Immagine, Bar chart, e N layer di attenzione\n",
        "    # Altezza per immagine, bar chart, e poi per ogni riga di ogni layer\n",
        "    height_ratios = [3, 2] + [2 * rows_per_layer] * num_decoder_layers\n",
        "    fig_height = sum(height_ratios)\n",
        "    fig_width = max(20, 2.5 * cols)\n",
        "\n",
        "    fig = plt.figure(figsize=(fig_width, fig_height))\n",
        "    gs_main = fig.add_gridspec(\n",
        "        num_main_rows, 1, height_ratios=height_ratios, hspace=1.2\n",
        "    )\n",
        "    fig.suptitle(\n",
        "        f\"Epoch {epoch}: Attention Visualization for PokÃ©mon #{pokemon_id} ({set_name.capitalize()})\",\n",
        "        fontsize=24,\n",
        "    )\n",
        "\n",
        "    # --- 1. Immagine Generata e Prompt ---\n",
        "    ax_main_img = fig.add_subplot(gs_main[0])\n",
        "    ax_main_img.imshow(img_tensor_cpu)\n",
        "    ax_main_img.set_title(\"Generated Image\", fontsize=18)\n",
        "    ax_main_img.text(\n",
        "        0.5,\n",
        "        -0.1,\n",
        "        f\"Prompt: {description}\",\n",
        "        ha=\"center\",\n",
        "        va=\"top\",\n",
        "        transform=ax_main_img.transAxes,\n",
        "        fontsize=14,\n",
        "        wrap=True,\n",
        "    )\n",
        "    ax_main_img.axis(\"off\")\n",
        "\n",
        "    # --- 2. Attenzione Iniziale per il Contesto (bar chart) ---\n",
        "    ax_initial_attn = fig.add_subplot(gs_main[1])\n",
        "    initial_weights_squeezed = initial_context_weights.squeeze().cpu().numpy()\n",
        "    token_strings = [t[\"token\"] for t in display_tokens]\n",
        "    token_indices = [t[\"index\"] for t in display_tokens]\n",
        "    relevant_weights = initial_weights_squeezed[token_indices]\n",
        "    ax_initial_attn.bar(\n",
        "        np.arange(len(token_strings)), relevant_weights, color=\"skyblue\"\n",
        "    )\n",
        "    ax_initial_attn.set_xticks(np.arange(len(token_strings)))\n",
        "    ax_initial_attn.set_xticklabels(token_strings, rotation=45, ha=\"right\", fontsize=10)\n",
        "    ax_initial_attn.set_title(\"Initial Context Attention (Global)\", fontsize=16)\n",
        "    ax_initial_attn.set_ylabel(\"Weight\", fontsize=12)\n",
        "    ax_initial_attn.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "    # --- 3. Attenzione per Strato del Decoder (griglie di heatmap) ---\n",
        "    for i, layer_attn_map in enumerate(decoder_attention_maps):\n",
        "        map_size_flat = layer_attn_map.shape[1]\n",
        "        map_side = int(np.sqrt(map_size_flat))\n",
        "        layer_title = (\n",
        "            f\"Decoder Cross-Attention Layer {i + 1} (Size: {map_side}x{map_side})\"\n",
        "        )\n",
        "        layer_attn_map_squeezed = layer_attn_map.squeeze(0).cpu()\n",
        "\n",
        "        # Seleziona solo le mappe di attenzione per i token che visualizziamo\n",
        "        relevant_attn_maps = layer_attn_map_squeezed[:, token_indices_to_display]\n",
        "\n",
        "        # Trova i valori min/max per questo strato per la colorbar\n",
        "        vmin = relevant_attn_maps.min()\n",
        "        vmax = relevant_attn_maps.max()\n",
        "\n",
        "        # Crea una subgrid per questo strato (con una colonna in piÃ¹ per la colorbar)\n",
        "        gs_layer = gs_main[2 + i].subgridspec(\n",
        "            rows_per_layer,\n",
        "            cols + 1,\n",
        "            wspace=0.2,\n",
        "            hspace=0.4,\n",
        "            width_ratios=[*([1] * cols), 0.1],\n",
        "        )\n",
        "\n",
        "        # Crea tutti gli assi per la griglia\n",
        "        axes_in_layer = [\n",
        "            fig.add_subplot(gs_layer[r, c])\n",
        "            for r in range(rows_per_layer)\n",
        "            for c in range(cols)\n",
        "        ]\n",
        "\n",
        "        # Usa la posizione del primo asse per il titolo\n",
        "        if axes_in_layer:\n",
        "            y_pos = axes_in_layer[0].get_position().y1\n",
        "            fig.text(\n",
        "                0.5,\n",
        "                y_pos + 0.01,\n",
        "                layer_title,\n",
        "                ha=\"center\",\n",
        "                va=\"bottom\",\n",
        "                fontsize=16,\n",
        "                weight=\"bold\",\n",
        "            )\n",
        "\n",
        "        for j, token_info in enumerate(display_tokens):\n",
        "            ax = axes_in_layer[j]\n",
        "            attn_for_token = layer_attn_map_squeezed[:, token_info[\"index\"]]\n",
        "            heatmap = attn_for_token.reshape(map_side, map_side)\n",
        "            im = ax.imshow(\n",
        "                heatmap, cmap=\"jet\", interpolation=\"nearest\", vmin=vmin, vmax=vmax\n",
        "            )\n",
        "            ax.set_title(f\"'{token_info['token']}'\", fontsize=12)\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "        # Aggiungi la colorbar\n",
        "        cax = fig.add_subplot(gs_layer[:, -1])\n",
        "        cbar = fig.colorbar(im, cax=cax)\n",
        "        cbar.ax.tick_params(labelsize=10)\n",
        "        cbar.set_label(\"Attention Weight\", rotation=270, labelpad=15, fontsize=12)\n",
        "\n",
        "        # Pulisce gli assi non usati nella griglia\n",
        "        for j in range(num_tokens, len(axes_in_layer)):\n",
        "            axes_in_layer[j].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout(rect=(0, 0.03, 1, 0.96))\n",
        "    save_path = os.path.join(\n",
        "        output_dir, f\"{epoch:03d}_{set_name}_attention_visualization.png\"\n",
        "    )\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "\n",
        "    if show_inline:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)\n",
        "\n",
        "def save_checkpoint(generator, discriminator, g_optimizer, d_optimizer, epoch, losses, path):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'generator_state_dict': generator.state_dict(),\n",
        "        'discriminator_state_dict': discriminator.state_dict(),\n",
        "        'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
        "        'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
        "        'losses': losses\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Checkpoint saved to {path}\")\n",
        "\n",
        "\n",
        "\n",
        "# Initialize models with enhanced discriminator\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)  # Now handles 256x256 images\n",
        "\n",
        "# Apply weight initialization\n",
        "generator.apply(weights_init)\n",
        "discriminator.apply(weights_init)\n",
        "\n",
        "print(\"âœ… Enhanced discriminator initialized for 256x256 images!\")\n",
        "\n",
        "# Setup optimizers\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(),\n",
        "                        lr=lr, betas=(beta1, beta2))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "# Loss function\n",
        "adv_criterion = nn.BCELoss().to(device)\n",
        "l1_criterion = nn.L1Loss().to(device)\n",
        "perc_criterion = VGGPerceptualLoss(device)\n",
        "sobel_criterion = SobelLoss().to(device)\n",
        "\n",
        "\n",
        "print(\"Models and optimizers initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. GAN Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Training history\n",
        "losses = {\n",
        "    'generator': [],\n",
        "    'discriminator': [],\n",
        "    'l1': [],\n",
        "    'perceptual': [],\n",
        "    'sobel': [],\n",
        "}\n",
        "\n",
        "# Validation history (separate tracking)\n",
        "val_losses = {\n",
        "    'l1': [],\n",
        "    'perceptual': [],\n",
        "    'sobel': [],\n",
        "    'total': [],\n",
        "}\n",
        "\n",
        "def validate_model(generator, val_loader, device, l1_criterion, perc_criterion, sobel_criterion):\n",
        "    \"\"\"\n",
        "    Validate the model on the validation set\n",
        "    Returns validation losses (dict)\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "\n",
        "    val_l1_loss = 0.0\n",
        "    val_perc_loss = 0.0\n",
        "    val_sobel_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # Move data to device\n",
        "            real_images = batch['image'].to(device)\n",
        "            text_ids = batch['text'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            # Generate images\n",
        "            generated_images = generator(text_ids, attention_mask)\n",
        "\n",
        "            # Calculate validation losses (no adversarial loss)\n",
        "            batch_l1_loss = l1_criterion(generated_images, real_images)\n",
        "            batch_perc_loss = perc_criterion(generated_images, real_images)\n",
        "            batch_sobel_loss = sobel_criterion(generated_images, real_images)\n",
        "\n",
        "            val_l1_loss += batch_l1_loss.item()\n",
        "            val_perc_loss += batch_perc_loss.item()\n",
        "            val_sobel_loss += batch_sobel_loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_val_l1 = val_l1_loss / num_batches\n",
        "    avg_val_perc = val_perc_loss / num_batches\n",
        "    avg_val_sobel = val_sobel_loss / num_batches\n",
        "    avg_val_total = avg_val_l1 + avg_val_perc + avg_val_sobel\n",
        "\n",
        "    # Set models back to training mode\n",
        "    generator.train()\n",
        "\n",
        "    return {\n",
        "        'l1': avg_val_l1,\n",
        "        'perceptual': avg_val_perc,\n",
        "        'sobel': avg_val_sobel,\n",
        "        'total': avg_val_total\n",
        "    }\n",
        "\n",
        "epoch = 0\n",
        "noise_dim = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Training parameters\n",
        "total_epochs = 150 # Reduced for faster training in demo\n",
        "display_interval = 1\n",
        "save_interval = 5\n",
        "clear_interval = 22\n",
        "\n",
        "# Generator update control parameters\n",
        "g_update_interval = 4  # Update generator every N epochs when loss is low\n",
        "g_loss_threshold = 0.3  # Threshold below which to reduce generator updates\n",
        "\n",
        "lambda_l1 = 0.0\n",
        "lambda_adv = 1.0\n",
        "lambda_perceptual = 0.0\n",
        "lambda_sobel = 0.0\n",
        "\n",
        "# Labels for real and fake data\n",
        "real_label = 1.0\n",
        "fake_label = 0.0\n",
        "\n",
        "print(\"Starting GAN training with validation...\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Batch size: {dataloader.batch_size}\")\n",
        "print(f\"Total epochs: {total_epochs}\")\n",
        "print(f\"Generator update interval: {g_update_interval} epochs when loss < {g_loss_threshold}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "for epoch in range(epoch, total_epochs):\n",
        "    epoch_g_loss = 0.0\n",
        "    epoch_d_loss = 0.0\n",
        "    epoch_l1_loss = 0.0\n",
        "    epoch_perc_loss = 0.0\n",
        "    epoch_sobel_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
        "\n",
        "    for i, batch in enumerate(progress_bar):\n",
        "        batch_size = batch['image'].size(0)\n",
        "\n",
        "        # Move data to device\n",
        "        real_images = batch['image'].to(device)\n",
        "        text_ids = batch['text'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "\n",
        "        # ==========================================\n",
        "        # Train Discriminator\n",
        "        # ==========================================\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Train with real images (256x256 - no resizing needed!)\n",
        "        real_labels = torch.full((batch_size, 1), real_label, device=device, dtype=torch.float)\n",
        "        real_output = discriminator(real_images, text_ids.detach(), attention_mask)\n",
        "        real_loss = adv_criterion(real_output, real_labels)\n",
        "\n",
        "        # Train with fake images (generator produces 256x256 images)\n",
        "        fake_images_full = generator(text_ids, attention_mask)\n",
        "        fake_labels = torch.full((batch_size, 1), fake_label, device=device, dtype=torch.float)\n",
        "        fake_output = discriminator(fake_images_full.detach(), text_ids.detach(), attention_mask)\n",
        "        fake_loss = adv_criterion(fake_output, fake_labels)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # ==========================================\n",
        "        # Train Generator (with conditional update)\n",
        "        # ==========================================\n",
        "\n",
        "        # Check if we should update the generator\n",
        "        update_generator = True\n",
        "        if len(losses['generator']) > 0:  # Check if we have previous loss history\n",
        "            n_recent_losses = min(5, len(losses['generator']))\n",
        "            avg_recent_g_loss = sum(losses['generator'][-n_recent_losses:]) / n_recent_losses\n",
        "            if avg_recent_g_loss < g_loss_threshold and (epoch + 1) % g_update_interval != 0:\n",
        "                update_generator = False\n",
        "\n",
        "        if update_generator:\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Generate fake images (generator produces 256x256 images)\n",
        "            fake_images_full = generator(text_ids, attention_mask)\n",
        "\n",
        "            # Adversarial loss (fool the discriminator) - no resizing needed!\n",
        "            fake_output = discriminator(fake_images_full, text_ids, attention_mask)\n",
        "            adversarial_loss = adv_criterion(fake_output, real_labels)\n",
        "\n",
        "            # Reconstruction loss (L1 loss with real images) - both are 256x256 now!\n",
        "            l1_loss = l1_criterion(fake_images_full, real_images) if lambda_l1 > 0 else torch.tensor(0.0, device=device)\n",
        "            perc_loss = perc_criterion(fake_images_full, real_images) if lambda_perceptual > 0 else torch.tensor(0.0, device=device)\n",
        "            sobel_loss = sobel_criterion(fake_images_full, real_images) if lambda_sobel > 0 else torch.tensor(0.0, device=device)\n",
        "\n",
        "            # Total generator loss\n",
        "            g_loss = lambda_l1 * l1_loss + \\\n",
        "                lambda_adv * adversarial_loss + \\\n",
        "                lambda_perceptual * perc_loss + \\\n",
        "                lambda_sobel * sobel_loss\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "        else:\n",
        "            # If not updating generator, still calculate losses for tracking\n",
        "            with torch.no_grad():\n",
        "                fake_images_full = generator(text_ids, attention_mask)\n",
        "                fake_output = discriminator(fake_images_full, text_ids, attention_mask)\n",
        "                adversarial_loss = adv_criterion(fake_output, real_labels)\n",
        "                l1_loss = l1_criterion(fake_images_full, real_images) if lambda_l1 > 0 else torch.tensor(0.0, device=device)\n",
        "                perc_loss = perc_criterion(fake_images_full, real_images) if lambda_perceptual > 0 else torch.tensor(0.0, device=device)\n",
        "                sobel_loss = sobel_criterion(fake_images_full, real_images) if lambda_sobel > 0 else torch.tensor(0.0, device=device)\n",
        "                g_loss = lambda_l1 * l1_loss + \\\n",
        "                    lambda_adv * adversarial_loss + \\\n",
        "                    lambda_perceptual * perc_loss + \\\n",
        "                    lambda_sobel * sobel_loss\n",
        "\n",
        "        # Update loss tracking\n",
        "        epoch_d_loss += d_loss.item()\n",
        "        epoch_l1_loss += l1_loss.item()\n",
        "        epoch_perc_loss += perc_loss.item()\n",
        "        epoch_sobel_loss += sobel_loss.item()\n",
        "        epoch_g_loss += g_loss.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'D_loss': f'{d_loss.item():.4f}',\n",
        "            'G_loss': f'{g_loss.item():.4f}',\n",
        "            'L1': f'{l1_loss.item():.4f}',\n",
        "            'Perceptual': f'{perc_loss.item():.4f}',\n",
        "            'Sobel': f'{sobel_loss.item():.4f}',\n",
        "            'G_updated': update_generator\n",
        "        })\n",
        "\n",
        "    # Calculate average losses for the epoch\n",
        "    avg_g_loss = epoch_g_loss / len(dataloader)\n",
        "    avg_d_loss = epoch_d_loss / len(dataloader)\n",
        "    avg_l1_loss = epoch_l1_loss / len(dataloader)\n",
        "    avg_perc_loss = epoch_perc_loss / len(dataloader)\n",
        "    avg_sobel_loss = epoch_sobel_loss / len(dataloader)\n",
        "\n",
        "    # Store losses\n",
        "    losses['generator'].append(avg_g_loss)\n",
        "    losses['discriminator'].append(avg_d_loss)\n",
        "    losses['l1'].append(avg_l1_loss)\n",
        "    losses['perceptual'].append(avg_perc_loss)\n",
        "    losses['sobel'].append(avg_sobel_loss)\n",
        "\n",
        "    # Run validation\n",
        "    print(f\"Running validation for epoch {epoch+1}...\")\n",
        "    validation_results = validate_model(generator, val_loader, device,\n",
        "                                      l1_criterion, perc_criterion, sobel_criterion)\n",
        "\n",
        "    # Store validation losses\n",
        "    val_losses['l1'].append(validation_results['l1'])\n",
        "    val_losses['perceptual'].append(validation_results['perceptual'])\n",
        "    val_losses['sobel'].append(validation_results['sobel'])\n",
        "    val_losses['total'].append(validation_results['total'])\n",
        "\n",
        "    if (epoch + 1) % clear_interval == 0:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{total_epochs}]\")\n",
        "    print(f\"  Train - D_loss: {avg_d_loss:.4f}, G_loss: {avg_g_loss:.4f}, L1: {avg_l1_loss:.4f}, Perceptual: {avg_perc_loss:.4f}, Sobel: {avg_sobel_loss:.4f}\")\n",
        "    print(f\"  Val   - L1: {validation_results['l1']:.4f}, Perceptual: {validation_results['perceptual']:.4f}, Sobel: {validation_results['sobel']:.4f}, Total: {validation_results['total']:.4f}\")\n",
        "\n",
        "    # Display generated images\n",
        "    if (epoch + 1) % display_interval == 0:\n",
        "        print(f\"\\nGenerating sample images at epoch {epoch+1}:\")\n",
        "        save_comparison_grid(epoch+1, generator, fixed_train_batch, \"train\", device, show_inline=True)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch + 1) % save_interval == 0:\n",
        "        checkpoint_path = f'models/checkpoint_epoch_{epoch+1}.pth'\n",
        "        # Include validation losses in checkpoint\n",
        "        all_losses = {'train': losses, 'val': val_losses}\n",
        "        save_checkpoint(generator, discriminator, optimizer_G, optimizer_D,\n",
        "                       epoch, all_losses, checkpoint_path)\n",
        "        save_comparison_grid(epoch+1, generator, fixed_val_batch, \"val\", device, show_inline=True)\n",
        "        save_attention_visualization(epoch+1, generator, tokenizer, fixed_train_batch, device, \"train\", show_inline=True)\n",
        "        save_attention_visualization(epoch+1, generator, tokenizer, fixed_val_batch, device, \"val\", show_inline=True)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nTraining completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Training Analysis and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced plot training losses using utils.py function\n",
        "save_plot_losses(\n",
        "    losses_g=losses['generator'],\n",
        "    losses_d=losses['discriminator'],\n",
        "    output_dir=\"training_output\",\n",
        "    show_inline=True\n",
        ")\n",
        "\n",
        "# Plot training vs validation losses for non-adversarial components\n",
        "# Convert to list of dicts format expected by save_plot_non_gan_losses\n",
        "train_losses_history = []\n",
        "val_losses_history = []\n",
        "\n",
        "for i in range(len(losses['l1'])):\n",
        "    train_losses_history.append({\n",
        "        'l1': losses['l1'][i],\n",
        "        'perceptual': losses['perceptual'][i],\n",
        "        'sobel': losses['sobel'][i],\n",
        "        'total': losses['l1'][i] + losses['perceptual'][i] + losses['sobel'][i]\n",
        "    })\n",
        "\n",
        "for i in range(len(val_losses['l1'])):\n",
        "    val_losses_history.append({\n",
        "        'l1': val_losses['l1'][i],\n",
        "        'perceptual': val_losses['perceptual'][i],\n",
        "        'sobel': val_losses['sobel'][i],\n",
        "        'total': val_losses['total'][i]\n",
        "    })\n",
        "\n",
        "save_plot_non_gan_losses(\n",
        "    train_losses_history=train_losses_history,\n",
        "    val_losses_history=val_losses_history,\n",
        "    output_dir=\"training_output\",\n",
        "    show_inline=True\n",
        ")\n",
        "\n",
        "# Print final statistics\n",
        "if losses['generator']:\n",
        "    print(f\"Final Train - Generator Loss: {losses['generator'][-1]:.4f}\")\n",
        "    print(f\"Final Train - Discriminator Loss: {losses['discriminator'][-1]:.4f}\")\n",
        "    print(f\"Final Train - L1 Loss: {losses['l1'][-1]:.4f}\")\n",
        "    print(f\"Final Train - Perceptual Loss: {losses['perceptual'][-1]:.4f}\")\n",
        "    print(f\"Final Train - Sobel Loss: {losses['sobel'][-1]:.4f}\")\n",
        "\n",
        "    if val_losses['l1']:\n",
        "        print(f\"Final Val   - L1 Loss: {val_losses['l1'][-1]:.4f}\")\n",
        "        print(f\"Final Val   - Perceptual Loss: {val_losses['perceptual'][-1]:.4f}\")\n",
        "        print(f\"Final Val   - Sobel Loss: {val_losses['sobel'][-1]:.4f}\")\n",
        "        print(f\"Final Val   - Total Loss: {val_losses['total'][-1]:.4f}\")\n",
        "else:\n",
        "    print(\"No training losses recorded yet.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a grid of final results\n",
        "print(\"Final Results - Generated Pokemon Sprites:\")\n",
        "batch = next(iter(dataloader))\n",
        "save_comparison_grid(0, generator, batch, \"final\", device, show_inline=True)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Interactive Demo with Custom Text Input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced interactive generation function with attention visualization\n",
        "def generate_pokemon_from_text(description, num_samples=4, show_attention=False):\n",
        "    \"\"\"Generate Pokemon sprites from custom text description with enhanced visualization\"\"\"\n",
        "    generator.eval()\n",
        "    text_encoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the description\n",
        "        tokens = tokenizer(\n",
        "            description,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Repeat for multiple samples\n",
        "        text_ids = tokens['input_ids'].repeat(num_samples, 1).to(device)\n",
        "        attention_mask = tokens['attention_mask'].repeat(num_samples, 1).to(device)\n",
        "\n",
        "        # Generate images (generator handles text encoding internally)\n",
        "        if show_attention:\n",
        "            generated_images, attention_maps, initial_weights = generator(\n",
        "                text_ids, attention_mask, return_attentions=True\n",
        "            )\n",
        "        else:\n",
        "            generated_images = generator(text_ids, attention_mask)\n",
        "\n",
        "        # Create batch format for visualization functions\n",
        "        fake_batch = {\n",
        "            'text': text_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'image': generated_images,  # Use generated as \"real\" for display\n",
        "            'description': [description] * num_samples,\n",
        "            'pokemon_name': [f\"Generated_{i+1}\" for i in range(num_samples)],\n",
        "            'idx': list(range(num_samples))\n",
        "        }\n",
        "\n",
        "        # Use enhanced comparison grid for better visualization\n",
        "        save_comparison_grid(\n",
        "            epoch=0,\n",
        "            model=generator,\n",
        "            batch=fake_batch,\n",
        "            set_name=\"custom\",\n",
        "            device=device,\n",
        "            output_dir=\"custom_generation\",\n",
        "            show_inline=True\n",
        "        )\n",
        "\n",
        "        # Show attention visualization if requested\n",
        "        if show_attention and attention_maps is not None:\n",
        "            print(\"\\nGenerating attention visualization...\")\n",
        "            # Create single-sample batch for attention visualization\n",
        "            single_batch = {\n",
        "                'text': text_ids[:1],\n",
        "                'attention_mask': attention_mask[:1],\n",
        "                'description': [description],\n",
        "                'idx': [0]\n",
        "            }\n",
        "            save_attention_visualization(\n",
        "                epoch=0,\n",
        "                model=generator,\n",
        "                tokenizer=tokenizer,\n",
        "                batch=single_batch,\n",
        "                device=device,\n",
        "                set_name=\"custom\",\n",
        "                output_dir=\"custom_generation\",\n",
        "                show_inline=True\n",
        "            )\n",
        "\n",
        "    generator.train()\n",
        "    text_encoder.train()\n",
        "\n",
        "# Simple visualization function for basic usage\n",
        "def simple_generate_pokemon(description, num_samples=4):\n",
        "    \"\"\"Simple generation without attention - for quick testing\"\"\"\n",
        "    generator.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokens = tokenizer(\n",
        "            description,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        text_ids = tokens['input_ids'].repeat(num_samples, 1).to(device)\n",
        "        attention_mask = tokens['attention_mask'].repeat(num_samples, 1).to(device)\n",
        "        generated_images = generator(text_ids, attention_mask)\n",
        "\n",
        "        # Simple matplotlib visualization\n",
        "        fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 3, 3))\n",
        "        if num_samples == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            img = denormalize_image(generated_images[i].cpu()).permute(1, 2, 0)\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].set_title(f\"Generated {i+1}\")\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.suptitle(f'Generated Pokemon: \"{description}\"', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    generator.train()\n",
        "\n",
        "# Test with custom descriptions using enhanced functions\n",
        "test_descriptions = [\n",
        "    \"A fire type pokemon with orange fur and a flame on its tail\",\n",
        "    \"A blue water type pokemon with bubbles\",\n",
        "    \"A grass type pokemon with green leaves and vines\",\n",
        "    \"An electric type pokemon with yellow fur and lightning bolts\",\n",
        "    \"A psychic type pokemon with purple coloring and mystical powers\"\n",
        "]\n",
        "\n",
        "print(\"ðŸ”¥ ENHANCED POKEMON GENERATION DEMO\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Testing both simple and enhanced generation functions...\\n\")\n",
        "\n",
        "# Test first description with enhanced visualization\n",
        "print(f\"âœ¨ ENHANCED Generation: {test_descriptions[0]}\")\n",
        "generate_pokemon_from_text(test_descriptions[0], num_samples=3, show_attention=False)\n",
        "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "# Test second description with simple visualization for comparison\n",
        "print(f\"âš¡ SIMPLE Generation: {test_descriptions[1]}\")\n",
        "simple_generate_pokemon(test_descriptions[1], num_samples=3)\n",
        "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "# Quick test of remaining descriptions with simple function\n",
        "print(\"ðŸŽ® Quick tests with simple generation:\")\n",
        "for desc in test_descriptions[2:]:\n",
        "    print(f\"\\nDescription: {desc}\")\n",
        "    simple_generate_pokemon(desc, num_samples=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ’¡ PRO TIP: Use show_attention=True for detailed attention analysis!\")\n",
        "print(\"Example: generate_pokemon_from_text('legendary dragon', show_attention=True)\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6.5. Enhanced Visualization Demo with Utils.py Functions\n",
        "\n",
        "# Test the enhanced visualization functions from utils.py\n",
        "print(\"ðŸŽ¨ ENHANCED VISUALIZATION DEMO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Test enhanced comparison grid with training data\n",
        "print(\"\\n1. Enhanced Comparison Grid:\")\n",
        "save_comparison_grid(\n",
        "    epoch=0,\n",
        "    model=generator,\n",
        "    batch=fixed_train_batch,\n",
        "    set_name=\"demo\",\n",
        "    device=device,\n",
        "    output_dir=\"demo_output\",\n",
        "    show_inline=True\n",
        ")\n",
        "\n",
        "# 2. Test attention visualization with a single sample\n",
        "print(\"\\n2. Attention Visualization (if attention is available):\")\n",
        "try:\n",
        "    save_attention_visualization(\n",
        "        epoch=0,\n",
        "        model=generator,\n",
        "        tokenizer=tokenizer,\n",
        "        batch=fixed_train_attention_batch,\n",
        "        device=device,\n",
        "        set_name=\"demo\",\n",
        "        output_dir=\"demo_output\",\n",
        "        show_inline=True\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Attention visualization not available: {e}\")\n",
        "    print(\"This is normal if the model doesn't have attention mechanisms enabled.\")\n",
        "\n",
        "# 3. Test enhanced loss plotting\n",
        "print(\"\\n3. Enhanced Loss Plotting:\")\n",
        "# Create some dummy loss data for demonstration\n",
        "demo_losses_g = [3.2, 2.8, 2.5, 2.2, 2.0, 1.8, 1.6, 1.5, 1.4, 1.3]\n",
        "demo_losses_d = [0.8, 0.7, 0.6, 0.65, 0.7, 0.68, 0.66, 0.64, 0.63, 0.62]\n",
        "demo_losses_recon = [0.4, 0.35, 0.3, 0.28, 0.25, 0.23, 0.22, 0.21, 0.20, 0.19]\n",
        "\n",
        "save_plot_losses(\n",
        "    losses_g=demo_losses_g,\n",
        "    losses_d=demo_losses_d,\n",
        "    losses_recon=demo_losses_recon,\n",
        "    output_dir=\"demo_output\",\n",
        "    show_inline=True\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Enhanced visualization functions successfully integrated!\")\n",
        "print(\"ðŸ“ All visualizations are saved to 'demo_output' and 'custom_generation' directories\")\n",
        "print(\"\\nðŸŽ¯ Available enhanced functions:\")\n",
        "print(\"  â€¢ save_comparison_grid() - Enhanced real vs generated comparison\")\n",
        "print(\"  â€¢ save_attention_visualization() - Detailed attention heatmaps\")\n",
        "print(\"  â€¢ save_plot_losses() - Professional loss plotting\")\n",
        "print(\"  â€¢ denormalize_image() - Proper image denormalization\")\n",
        "print(\"  â€¢ generate_pokemon_from_text() - Now with attention visualization!\")\n",
        "print(\"\\nðŸ’¡ Usage examples:\")\n",
        "print(\"  generate_pokemon_from_text('fire dragon', num_samples=4, show_attention=True)\")\n",
        "print(\"  simple_generate_pokemon('electric mouse', num_samples=3)  # For quick testing\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Model Analysis and Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final model summary and analysis\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PIKAPIKAGEN: FINAL MODEL ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\\\nðŸ“Š MODEL STATISTICS:\")\n",
        "print(f\"Generator parameters: {count_parameters(generator):,}\")\n",
        "print(f\"Discriminator parameters: {count_parameters(discriminator):,}\")\n",
        "print(f\"Total parameters: {count_parameters(generator) + count_parameters(discriminator):,}\")\n",
        "\n",
        "print(f\"\\\\nðŸ“ˆ TRAINING STATISTICS:\")\n",
        "print(f\"Total epochs trained: {len(losses['generator'])}\")\n",
        "print(f\"Final Generator Loss: {losses['generator'][-1]:.4f}\")\n",
        "print(f\"Final Discriminator Loss: {losses['discriminator'][-1]:.4f}\")\n",
        "print(f\"Final Reconstruction Loss: {losses['reconstruction'][-1]:.4f}\")\n",
        "\n",
        "print(f\"\\\\nðŸŽ¯ MODEL CAPABILITIES:\")\n",
        "print(\"âœ… Text-to-Image Generation with Attention\")\n",
        "print(\"âœ… BERT-mini Text Encoding (Fine-tuned)\")\n",
        "print(\"âœ… Adversarial Training with Reconstruction Loss\")\n",
        "print(\"âœ… Interactive Custom Text Generation\")\n",
        "print(\"âœ… Real-time Training Visualization\")\n",
        "\n",
        "print(f\"\\\\nðŸ“ ARCHITECTURE SUMMARY:\")\n",
        "print(\"â€¢ Text Encoder: Transformer-based with pre-trained BERT-mini embeddings\")\n",
        "print(\"â€¢ Generator: CNN decoder with multi-layer attention mechanism\")\n",
        "print(\"â€¢ Discriminator: CNN discriminator with text conditioning\")\n",
        "print(\"â€¢ Attention: Allows selective focus on text features during generation\")\n",
        "print(\"â€¢ Loss: Adversarial + Reconstruction (MSE) loss combination\")\n",
        "\n",
        "print(f\"\\\\nðŸ”¥ SUCCESS METRICS:\")\n",
        "print(\"â€¢ Successfully generates Pokemon sprites from text descriptions\")\n",
        "print(\"â€¢ Attention mechanism enables fine-grained text-image alignment\")\n",
        "print(\"â€¢ BERT-mini fine-tuning improves domain-specific understanding\")\n",
        "print(\"â€¢ Combined loss function balances realism and text fidelity\")\n",
        "print(\"â€¢ Real-time visualization shows training progress\")\n",
        "\n",
        "print(\"\\\\nâœ¨ The PikaPikaGen model is now ready for Pokemon sprite generation!\")\n",
        "print(\"ðŸŽ® Try generating your own Pokemon with custom descriptions!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Show final generation with interactive input\n",
        "print(\"\\\\nðŸŽ¯ INTERACTIVE DEMO:\")\n",
        "print(\"Try this: generate_pokemon_from_text('Your custom Pokemon description here!')\")\n",
        "print(\"\\\\nExample: generate_pokemon_from_text('A dragon type pokemon with silver wings and red eyes', num_samples=4)\")\n",
        "\n",
        "# Quick demonstration\n",
        "generate_pokemon_from_text(\"A legendary fire dragon pokemon with golden scales\", num_samples=4)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
