{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# PikaPikaGen: Text-to-Image Pokemon Sprite Generation with GAN\n",
    "\n",
    "This notebook implements a Generative Adversarial Network (GAN) for generating Pokemon sprites from textual descriptions, based on the encoder-decoder architecture with attention mechanism described in the instructions.\n",
    "\n",
    "## Architecture Overview:\n",
    "- **Text Encoder**: Transformer-based encoder with BERT-mini embeddings\n",
    "- **Generator**: CNN decoder with attention mechanism (from instructions)\n",
    "- **Discriminator**: CNN discriminator for adversarial training\n",
    "- **Attention Mechanism**: Links text features with image generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!pip install torch torchvision transformers pandas pillow requests matplotlib tqdm ipywidgets gradio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download utility\n",
    "def reporthook(block_num, block_size, total_size):\n",
    "    if block_num % 16384 == 0:\n",
    "        print(f\"Downloading... {block_num * block_size / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "def download_dataset_if_not_exists():\n",
    "    dataset_dir = \"dataset\"\n",
    "    pokedex_main_dir = os.path.join(dataset_dir, \"pokedex-main\")\n",
    "    zip_url = \"https://github.com/cristobalmitchell/pokedex/archive/refs/heads/main.zip\"\n",
    "    zip_path = \"pokedex_main.zip\"\n",
    "\n",
    "    # Check if dataset/pokedex-main already exists\n",
    "    if os.path.exists(pokedex_main_dir):\n",
    "        print(f\"{pokedex_main_dir} already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    # Create dataset directory if it doesn't exist\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Download the zip file\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(zip_url, zip_path, reporthook)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "    # Extract the zip file into the dataset directory\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "    # Optionally, remove the zip file after extraction\n",
    "    os.remove(zip_path)\n",
    "\n",
    "# Download the dataset\n",
    "download_dataset_if_not_exists()\n",
    "print(\"Dataset ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class AugmentationPipeline:\n",
    "    def __init__(self, p=0.8):\n",
    "        self.p = p\n",
    "        self.transforms = T.RandomApply([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "            # Applica trasformazioni affini (rotazione/scala) solo il 50% delle volte.\n",
    "            # Ho ridotto leggermente l'intensità (degrees=10).\n",
    "            T.RandomApply([\n",
    "                T.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05), fill=1)\n",
    "            ], p=0.5),\n",
    "\n",
    "            # Applica ColorJitter solo il 50% delle volte.\n",
    "            # I parametri sono già abbastanza bassi, quindi li manteniamo.\n",
    "            T.RandomApply([\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n",
    "            ], p=0.5),\n",
    "\n",
    "            # --- Passo 4: RandomErasing (su Tensore) ---\n",
    "            # Ridotto la probabilità di applicazione.\n",
    "            # È una tecnica forte, meglio usarla con parsimonia per iniziare.\n",
    "            T.RandomErasing(p=0.15, scale=(0.02, 0.1), ratio=(0.3, 3.3), value='random'),\n",
    "        ], p=self.p)\n",
    "\n",
    "    def apply(self, images):\n",
    "        return self.transforms(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Pokemon Dataset Class with modular augmentation support\n",
    "class PokemonDataset(Dataset):\n",
    "    def __init__(self, tokenizer, csv_path=\"dataset/pokedex-main/data/pokemon.csv\",\n",
    "                 image_dir=\"dataset/pokedex-main/images/small_images\",\n",
    "                 max_length=128, augmentation_pipeline=None):\n",
    "        \"\"\"\n",
    "        Dataset per Pokemon: testo (descrizione) -> immagine (sprite)\n",
    "        Enhanced with modular augmentation pipeline support\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path, encoding='utf-16 LE', delimiter='\\t')\n",
    "        self.image_dir = Path(image_dir)\n",
    "        print(f\"Dataset caricato: {len(self.df)} Pokemon con descrizioni e immagini\")\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augmentation_pipeline = augmentation_pipeline\n",
    "\n",
    "        if self.augmentation_pipeline is not None:\n",
    "            self.final_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((256, 256), antialias=True),\n",
    "                self.augmentation_pipeline,\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalizza a [-1, 1]\n",
    "            ])\n",
    "        else:\n",
    "            self.final_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((256, 256), antialias=True),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalizza a [-1, 1]\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Restituisce il numero totale di campioni\"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Restituisce un singolo campione: (testo_tokenizzato, immagine_tensor)\n",
    "        Full implementation matching pokemon_dataset.py\n",
    "        \"\"\"\n",
    "        # Ottieni la riga corrispondente\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # === PREPROCESSING DEL TESTO ===\n",
    "        description = str(row['description'])\n",
    "\n",
    "        # Tokenizza il testo\n",
    "        encoded = self.tokenizer(\n",
    "            description,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Estrai token_ids e attention_mask\n",
    "        text_ids = encoded['input_ids'].squeeze(0)  # Rimuovi la dimensione batch\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "\n",
    "        # === CARICAMENTO E PREPROCESSING DELL'IMMAGINE ===\n",
    "        # Costruisce il percorso dell'immagine\n",
    "        image_filename = f\"{row['national_number']:03d}.png\"\n",
    "        image_path = self.image_dir/image_filename\n",
    "\n",
    "        # Carica l'immagine\n",
    "        image_rgba = Image.open(image_path).convert('RGBA')\n",
    "\n",
    "        # Gestisce la trasparenza: ricombina l'immagine con uno sfondo bianco\n",
    "        background = Image.new('RGB', image_rgba.size, (255, 255, 255))\n",
    "        background.paste(image_rgba, mask=image_rgba.split()[-1])\n",
    "\n",
    "        # Applica le trasformazioni finali (ToTensor, Resize, Normalize)\n",
    "        image_tensor = self.final_transform(background)\n",
    "\n",
    "        # Costruisce il risultato (matches pokemon_dataset.py structure)\n",
    "        sample = {\n",
    "            'text': text_ids,\n",
    "            'image': image_tensor,\n",
    "            'description': description,  # Per debug o visualizzazione\n",
    "            'pokemon_name': row['english_name'],\n",
    "            'idx': idx,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "def create_training_setup(tokenizer, train_val_split, batch_size, num_workers=0,\n",
    "                         num_viz_samples=4, random_seed=42, train_augmentation_pipeline=None):\n",
    "    \"\"\"\n",
    "    Crea un setup completo per il training con dataset, dataloader e batch fissi per visualizzazione.\n",
    "    Enhanced with modular augmentation pipeline support\n",
    "    \"\"\"\n",
    "    from torch.utils.data import random_split, TensorDataset, Subset\n",
    "\n",
    "    # --- Creazione dei Dataset ---\n",
    "    # Crea un'istanza per il training (con augmentazione) e la validazione (senza augmentazione)\n",
    "    train_full_dataset = PokemonDataset(tokenizer=tokenizer, augmentation_pipeline=train_augmentation_pipeline)\n",
    "    val_full_dataset = PokemonDataset(tokenizer=tokenizer, augmentation_pipeline=None)  # No augmentation for validation\n",
    "\n",
    "    # --- Divisione deterministica degli indici ---\n",
    "    assert len(train_full_dataset) == len(val_full_dataset)\n",
    "    dataset_size = len(train_full_dataset)\n",
    "    train_size = int(train_val_split * dataset_size)\n",
    "    val_size = dataset_size - train_size\n",
    "\n",
    "    train_indices_subset, val_indices_subset = random_split(\n",
    "        TensorDataset(torch.arange(dataset_size)),\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(random_seed),\n",
    "    )\n",
    "\n",
    "    train_dataset = Subset(train_full_dataset, train_indices_subset.indices)\n",
    "    val_dataset = Subset(val_full_dataset, val_indices_subset.indices)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # --- Creazione deterministica dei batch per la visualizzazione ---\n",
    "    vis_generator = torch.Generator().manual_seed(random_seed)\n",
    "    fixed_train_batch = next(\n",
    "        iter(\n",
    "            DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=num_viz_samples,\n",
    "                shuffle=True,\n",
    "                generator=vis_generator,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    fixed_val_batch = next(\n",
    "        iter(DataLoader(val_dataset, batch_size=num_viz_samples, shuffle=False))\n",
    "    )  # la validazione non ha shuffle\n",
    "\n",
    "    vis_generator.manual_seed(random_seed)  # Reset per coerenza\n",
    "    fixed_train_attention_batch = next(\n",
    "        iter(\n",
    "            DataLoader(\n",
    "                train_dataset, batch_size=1, shuffle=True, generator=vis_generator\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    fixed_val_attention_batch = next(\n",
    "        iter(DataLoader(val_dataset, batch_size=1, shuffle=False))\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'fixed_train_batch': fixed_train_batch,\n",
    "        'fixed_val_batch': fixed_val_batch,\n",
    "        'fixed_train_attention_batch': fixed_train_attention_batch,\n",
    "        'fixed_val_attention_batch': fixed_val_attention_batch,\n",
    "        'train_dataset': train_dataset,\n",
    "        'val_dataset': val_dataset,\n",
    "    }\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-mini')\n",
    "\n",
    "# train_augmentation_pipeline = AugmentationPipeline()\n",
    "# Create the complete training setup using the function from pokemon_dataset.py\n",
    "print(\"Creating training setup with train/val split and fixed batches...\")\n",
    "training_setup = create_training_setup(\n",
    "    tokenizer=tokenizer,\n",
    "    train_val_split=0.9,\n",
    "    batch_size=16,\n",
    "    num_workers=0,\n",
    "    num_viz_samples=4,\n",
    "    random_seed=42,\n",
    "    train_augmentation_pipeline=None\n",
    ")\n",
    "\n",
    "# Extract components\n",
    "train_loader = training_setup['train_loader']\n",
    "val_loader = training_setup['val_loader']\n",
    "fixed_train_batch = training_setup['fixed_train_batch']\n",
    "fixed_val_batch = training_setup['fixed_val_batch']\n",
    "fixed_train_attention_batch = training_setup['fixed_train_attention_batch']\n",
    "fixed_val_attention_batch = training_setup['fixed_val_attention_batch']\n",
    "train_dataset = training_setup['train_dataset']\n",
    "val_dataset = training_setup['val_dataset']\n",
    "\n",
    "print(f\"Training setup complete!\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Train loader batches: {len(train_loader)}\")\n",
    "print(f\"Val loader batches: {len(val_loader)}\")\n",
    "\n",
    "# Test the training setup with fixed batches\n",
    "print(f\"\\nFixed batch shapes:\")\n",
    "print(f\"  Train batch - Images: {fixed_train_batch['image'].shape}\")\n",
    "print(f\"  Train batch - Text: {fixed_train_batch['text'].shape}\")\n",
    "print(f\"  Train batch - Attention: {fixed_train_batch['attention_mask'].shape}\")\n",
    "print(f\"  Val batch - Images: {fixed_val_batch['image'].shape}\")\n",
    "\n",
    "# Display sample images from fixed batches\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i in range(4):\n",
    "    # Fixed train batch images\n",
    "    img = (fixed_train_batch['image'][i] + 1) / 2.0  # Denormalize\n",
    "    axes[0, i].imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[0, i].set_title(f\"Train: {fixed_train_batch['pokemon_name'][i]}\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Fixed val batch images\n",
    "    img = (fixed_val_batch['image'][i] + 1) / 2.0  # Denormalize\n",
    "    axes[1, i].imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[1, i].set_title(f\"Val: {fixed_val_batch['pokemon_name'][i]}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Fixed Batches for Training Visualization\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Set the main dataloader to use train_loader for consistency\n",
    "dataloader = train_loader\n",
    "sample_batch = fixed_train_batch\n",
    "\n",
    "print(f\"\\n✅ Dataset and batches loaded successfully from pokemon_dataset.py functionality!\")\n",
    "print(f\"Ready for training with proper train/val split and fixed visualization batches.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Demonstrate that augmentations are working and different for the same Pokemon\n",
    "print(f\"\\n🔄 AUGMENTATION VERIFICATION:\")\n",
    "print(\"Testing that augmentations produce different results for the same Pokemon...\")\n",
    "\n",
    "train_augmentation_pipeline = AugmentationPipeline()\n",
    "\n",
    "# Pick the first Pokemon from the training dataset and show it with different augmentations\n",
    "test_pokemon_idx = 0\n",
    "original_dataset = PokemonDataset(tokenizer=tokenizer, augmentation_pipeline=None)  # No augmentation\n",
    "augmented_dataset = PokemonDataset(tokenizer=tokenizer, augmentation_pipeline=train_augmentation_pipeline.transforms)  # With augmentation\n",
    "\n",
    "# Get the same Pokemon multiple times to see different augmentations\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "fig.suptitle(\"Augmentation Verification: Same Pokemon with Different Augmentations\", fontsize=16)\n",
    "\n",
    "# Row 0: Original (no augmentation) - same image repeated\n",
    "original_sample = original_dataset[test_pokemon_idx]\n",
    "for i in range(5):\n",
    "    img = (original_sample['image'] + 1) / 2.0  # Denormalize\n",
    "    axes[0, i].imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[0, i].set_title(f\"Original {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Row 1: Augmented (should be different each time)\n",
    "for i in range(5):\n",
    "    augmented_sample = augmented_dataset[test_pokemon_idx]  # Same index, different augmentation\n",
    "    img = (augmented_sample['image'] + 1) / 2.0  # Denormalize\n",
    "    axes[1, i].imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[1, i].set_title(f\"Augmented {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# Add row labels\n",
    "axes[0, 0].text(-0.1, 0.5, 'No Augmentation\\n(Should be identical)',\n",
    "                ha='center', va='center', rotation='vertical',\n",
    "                fontsize=12, transform=axes[0, 0].transAxes)\n",
    "axes[1, 0].text(-0.1, 0.5, 'With Augmentation\\n(Should be different)',\n",
    "                ha='center', va='center', rotation='vertical',\n",
    "                fontsize=12, transform=axes[1, 0].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Pokemon tested: {original_sample['pokemon_name']} (#{original_sample['idx']})\")\n",
    "print(f\"Description: {original_sample['description'][:60]}...\")\n",
    "print(f\"✅ If augmentations are working correctly, the bottom row should show different variations!\")\n",
    "print(f\"   - Look for differences in: rotation, translation, color, brightness, horizontal flip\")\n",
    "print(f\"   - The top row should be identical (no augmentation)\")\n",
    "print(f\"   - This proves augmentations will be different at each epoch for the same Pokemon!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Model Architecture Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder per processare il testo.\n",
    "    Usa gli embedding di bert-mini e li fa passare in un Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"prajjwal1/bert-mini\", fine_tune_embeddings=True):\n",
    "        super().__init__()\n",
    "        # Carica il modello bert-mini pre-addestrato per estrarre gli embedding\n",
    "        bert_mini_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Estrae lo strato di embedding\n",
    "        self.embedding = bert_mini_model.embeddings\n",
    "\n",
    "        # Imposta se fare il fine-tuning degli embedding durante il training\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = fine_tune_embeddings\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=256, nhead=4, dim_feedforward=1024, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "\n",
    "    def forward(self, token_ids, attention_mask=None):\n",
    "        # 1. Ottieni gli embedding dai token ID\n",
    "        # Shape: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "        embedded_text = self.embedding(token_ids)\n",
    "\n",
    "        # 2. Prepara la maschera di padding per il TransformerEncoder\n",
    "        # La maschera di HuggingFace è 1 per i token reali, 0 per il padding.\n",
    "        # TransformerEncoder si aspetta True per le posizioni da ignorare (padding).\n",
    "        src_key_padding_mask = None\n",
    "        if attention_mask is not None:\n",
    "            src_key_padding_mask = (attention_mask == 0)\n",
    "\n",
    "        # 3. Passa gli embedding attraverso il Transformer Encoder con la maschera\n",
    "        # Shape: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, embedding_dim)\n",
    "        encoder_output = self.transformer_encoder(\n",
    "            src=embedded_text,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        return encoder_output\n",
    "\n",
    "\n",
    "class ImageCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Modulo di Cross-Attention.\n",
    "    Permette a una sequenza di query (dall'immagine) di \"prestare attenzione\"\n",
    "    a una sequenza di key/value (dal testo), gestendo internamente\n",
    "    il reshaping dei tensori e la maschera di padding.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, image_features, text_features, key_padding_mask=None):\n",
    "        # query: (B, C, H, W) - Feature dell'immagine (spaziale)\n",
    "        # key/value: (B, seq_len, embed_dim) - Output dell'encoder di testo\n",
    "        # key_padding_mask: (B, seq_len) - Maschera dal tokenizer\n",
    "\n",
    "        B, C, H, W = image_features.shape\n",
    "\n",
    "        # 1. Prepara la query (feature dell'immagine)\n",
    "        # Reshape da spaziale a sequenza: (B, C, H, W) -> (B, H*W, C)\n",
    "        query_seq = image_features.view(B, C, H * W).permute(0, 2, 1)\n",
    "        query_norm = self.layer_norm(query_seq)\n",
    "\n",
    "        # 2. Prepara la maschera di padding per l'attenzione\n",
    "        # La maschera di HuggingFace è 1 per i token reali, 0 per il padding.\n",
    "        # MultiheadAttention si aspetta True per le posizioni da ignorare.\n",
    "        if key_padding_mask is not None:\n",
    "            mask = (key_padding_mask == 0)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        # 3. Applica l'attenzione\n",
    "        attn_output, attn_weights = self.attention(\n",
    "            query=query_norm,\n",
    "            key=text_features,\n",
    "            value=text_features,\n",
    "            key_padding_mask=mask,\n",
    "            need_weights=True\n",
    "        )\n",
    "        # attn_output: (B, H*W, C)\n",
    "\n",
    "        # 4. Riconverti l'output nella forma spaziale originale\n",
    "        # (B, H*W, C) -> (B, C, H*W) -> (B, C, H, W)\n",
    "        attn_output_spatial = attn_output.permute(0, 2, 1).view(B, C, H, W)\n",
    "\n",
    "        return attn_output_spatial, attn_weights\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Blocco del Generatore come da istruzioni:\n",
    "    Attenzione (opzionale) -> Fusione -> Upsampling (ConvTranspose) -> Normalizzazione -> Attivazione.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_attention=True, text_embed_dim=256, nhead=4):\n",
    "        super().__init__()\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "        if self.use_attention:\n",
    "            # Se in_channels è diverso da text_embed_dim, aggiungi una conv 1x1 per adattare le dimensioni\n",
    "            if in_channels != text_embed_dim:\n",
    "                self.channel_adapter = nn.Conv2d(in_channels, text_embed_dim, kernel_size=1, bias=False)\n",
    "                attention_dim = text_embed_dim\n",
    "            else:\n",
    "                self.channel_adapter = None\n",
    "                attention_dim = in_channels\n",
    "\n",
    "            self.cross_attention = ImageCrossAttention(embed_dim=attention_dim, num_heads=nhead)\n",
    "            # Nuova convolution per fondere le feature dell'immagine con il contesto del testo\n",
    "            self.fusion_conv = nn.Conv2d(attention_dim * 2, in_channels, kernel_size=1, bias=False)\n",
    "\n",
    "        # Blocco di upsampling come da istruzioni\n",
    "        self.upsample_block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels), # Equivalente a LayerNorm per feature map (N, C, H, W)\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_output=None, attention_mask=None):\n",
    "        attn_weights = None\n",
    "        if self.use_attention:\n",
    "            if encoder_output is None or attention_mask is None:\n",
    "                raise ValueError(\"encoder_output and attention_mask must be provided for attention.\")\n",
    "\n",
    "            # Adatta le dimensioni se necessario\n",
    "            if self.channel_adapter is not None:\n",
    "                x_adapted = self.channel_adapter(x)\n",
    "            else:\n",
    "                x_adapted = x\n",
    "\n",
    "            attn_output, attn_weights = self.cross_attention(\n",
    "                image_features=x_adapted,\n",
    "                text_features=encoder_output,\n",
    "                key_padding_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            # Concatena le feature adattate (x_adapted) con il contesto (attn_output)\n",
    "            # e le fonde con una convoluzione 1x1.\n",
    "            fused_features = torch.cat([x_adapted, attn_output], dim=1) # Shape: (B, 2*attention_dim, H, W)\n",
    "            skip = self.fusion_conv(fused_features) # Shape: (B, in_channels, H, W)\n",
    "            x = x + skip  # Shape: (B, in_channels, H, W)\n",
    "\n",
    "\n",
    "        # Apply the U-Net style sequence\n",
    "        x = self.upsample_block(x)\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class ImageDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder CNN (Generatore) che sintetizza l'immagine.\n",
    "    Questa versione usa l'attenzione per-step fin dall'inizio.\n",
    "    \"\"\"\n",
    "    def __init__(self, noise_dim, text_embed_dim, final_image_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Meccanismo per calcolare i punteggi di attenzione per il contesto iniziale.\n",
    "        self.initial_context_scorer = nn.Sequential(\n",
    "            nn.Linear(in_features=text_embed_dim, out_features=512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=512, out_features=1)\n",
    "            # Il Softmax viene applicato nel forward pass per poter usare la maschera\n",
    "        )\n",
    "\n",
    "        # Proiezione lineare iniziale a una feature map 4x4.\n",
    "        self.initial_projection = nn.Sequential(\n",
    "            nn.Linear(noise_dim + text_embed_dim, 256 * 4 * 4),\n",
    "            nn.GroupNorm(1, 256 * 4 * 4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Blocchi condivisi per entrambe le risoluzioni (fino a 64x64)\n",
    "        self.blocks_64 = nn.ModuleList([\n",
    "            # Input: (B, 256, 4, 4)   -> Output: (B, 256, 8, 8)\n",
    "            DecoderBlock(in_channels=256, out_channels=256, use_attention=True),\n",
    "            # Input: (B, 256, 8, 8)   -> Output: (B, 256, 16, 16)\n",
    "            DecoderBlock(in_channels=256, out_channels=256, use_attention=True),\n",
    "            # Input: (B, 256, 16, 16)  -> Output: (B, 128, 32, 32)\n",
    "            DecoderBlock(in_channels=256, out_channels=128, use_attention=True),\n",
    "            # Input: (B, 128, 32, 32)  -> Output: (B, 64, 64, 64)\n",
    "            DecoderBlock(in_channels=128, out_channels=64, use_attention=False),\n",
    "        ])\n",
    "\n",
    "        # Blocchi aggiuntivi solo per 256x256 (da 64x64 a 256x256)\n",
    "        self.blocks_256 = nn.ModuleList([\n",
    "            # Input: (B, 64, 64, 64)  -> Output: (B, 32, 128, 128)\n",
    "            DecoderBlock(in_channels=64, out_channels=32, use_attention=False),\n",
    "            # Input: (B, 32, 128, 128) -> Output: (B, 16, 256, 256)\n",
    "            DecoderBlock(in_channels=32, out_channels=16, use_attention=False),\n",
    "        ])\n",
    "\n",
    "        # Layer finale per portare ai canali RGB - 256x256\n",
    "        # Input: (B, 16, 256, 256) -> Output: (B, 3, 256, 256)\n",
    "        self.final_conv_256 = nn.Conv2d(16, final_image_channels, kernel_size=3, padding=1)\n",
    "        self.final_activation_256 = nn.Tanh()\n",
    "\n",
    "        # Layer finale per portare ai canali RGB - 64x64\n",
    "        # Input: (B, 64, 64, 64) -> Output: (B, 3, 64, 64)\n",
    "        self.final_conv_64 = nn.Conv2d(64, final_image_channels, kernel_size=3, padding=1)\n",
    "        self.final_activation_64 = nn.Tanh()\n",
    "\n",
    "    def forward(self, noise, encoder_output_full, attention_mask):\n",
    "        # noise.shape: (B, noise_dim)\n",
    "        # encoder_output_full.shape: (B, seq_len, text_embed_dim)\n",
    "        # attention_mask.shape: (B, seq_len)\n",
    "\n",
    "        # 1. Calcola il vettore di contesto iniziale con una media pesata (ATTENZIONE #1)\n",
    "        # Calcola i punteggi (logits) per ogni token del testo\n",
    "        attn_scores = self.initial_context_scorer(encoder_output_full)\n",
    "\n",
    "        # Applica la maschera di padding prima del softmax.\n",
    "        # Imposta i punteggi dei token di padding a -infinito.\n",
    "        if attention_mask is not None:\n",
    "            # La maschera è (B, seq_len), i punteggi (B, seq_len, 1)\n",
    "            # Il broadcast si occupa di allineare le dimensioni.\n",
    "            attn_scores.masked_fill_(attention_mask.unsqueeze(-1) == 0, -1e9)\n",
    "\n",
    "        # Ora applica il softmax per ottenere i pesi.\n",
    "        # attention_weights.shape: (B, seq_len, 1)\n",
    "        attention_weights = torch.softmax(attn_scores, dim=1)\n",
    "\n",
    "        # Calcola il contesto come media pesata degli output dell'encoder.\n",
    "        # context_vector.shape: (B, text_embed_dim)\n",
    "        context_vector = torch.sum(attention_weights * encoder_output_full, dim=1)\n",
    "\n",
    "        # 2. Prepara il vettore di input iniziale per la proiezione\n",
    "        #    Si usa direttamente il rumore 'noise' invece del vettore di stile 'w'\n",
    "        # initial_input.shape: (B, noise_dim + text_embed_dim)\n",
    "        initial_input = torch.cat([noise, context_vector], dim=1)\n",
    "\n",
    "        # 3. Proietta e rimodella\n",
    "        # x.shape: (B, 256 * 4 * 4)\n",
    "        x = self.initial_projection(initial_input)\n",
    "        # x.shape: (B, 256, 4, 4)\n",
    "        x = x.view(x.size(0), 256, 4, 4)\n",
    "\n",
    "        # 5. Passa attraverso i blocchi del decoder\n",
    "        attention_maps = []\n",
    "\n",
    "        # Percorso condiviso per entrambe le risoluzioni (fino a 64x64)\n",
    "        for block in self.blocks_64:\n",
    "            encoder_ctx = encoder_output_full if block.use_attention else None\n",
    "            mask_ctx = attention_mask if block.use_attention else None\n",
    "            x, attn_weights = block(x, encoder_ctx, mask_ctx)\n",
    "            if attn_weights is not None:\n",
    "                attention_maps.append(attn_weights)\n",
    "\n",
    "        # A questo punto x ha dimensione (B, 64, 64, 64)\n",
    "\n",
    "        # Percorso per 64x64 (usa direttamente x_shared)\n",
    "        image_64 = self.final_conv_64(x)\n",
    "        image_64 = self.final_activation_64(image_64)\n",
    "\n",
    "        # Percorso per 256x256 (continua con blocchi aggiuntivi)\n",
    "        for block in self.blocks_256:\n",
    "            encoder_ctx = encoder_output_full if block.use_attention else None\n",
    "            mask_ctx = attention_mask if block.use_attention else None\n",
    "            x, attn_weights = block(x, encoder_ctx, mask_ctx)\n",
    "            if attn_weights is not None:\n",
    "                attention_maps.append(attn_weights)\n",
    "\n",
    "        # 6. Layer finale per 256x256\n",
    "        # x_256.shape: (B, 16, 256, 256) -> (B, 3, 256, 256)\n",
    "        image_256 = self.final_conv_256(x)\n",
    "        image_256 = self.final_activation_256(image_256)\n",
    "\n",
    "        # 7. Layer finale per 64x64\n",
    "        # x_64.shape: (B, 64, 64, 64) -> (B, 3, 64, 64)\n",
    "\n",
    "        return image_256, image_64, attention_maps, attention_weights\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Modello completo che unisce Encoder e Decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, text_encoder_model_name=\"prajjwal1/bert-mini\", noise_dim=100):\n",
    "        super().__init__()\n",
    "        self.text_encoder = TextEncoder(\n",
    "            model_name=text_encoder_model_name,\n",
    "        )\n",
    "\n",
    "        text_embed_dim = 256\n",
    "\n",
    "        self.image_decoder = ImageDecoder(\n",
    "            noise_dim=noise_dim,\n",
    "            text_embed_dim=text_embed_dim\n",
    "        )\n",
    "\n",
    "        self.noise_dim = noise_dim\n",
    "\n",
    "    def forward(self, token_ids, attention_mask, return_attentions=False):\n",
    "        # token_ids.shape: (batch_size, seq_len)\n",
    "        # attention_mask.shape: (batch_size, seq_len)\n",
    "        # Genera rumore casuale per il batch\n",
    "        batch_size = token_ids.size(0)\n",
    "        # noise.shape: (batch_size, noise_dim)\n",
    "        noise = torch.randn(batch_size, self.noise_dim, device=token_ids.device)\n",
    "\n",
    "        # 1. Codifica il testo per ottenere i vettori di ogni parola\n",
    "        # encoder_output.shape: (batch_size, seq_len, text_embed_dim)\n",
    "        encoder_output = self.text_encoder(token_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # 2. Genera l'immagine usando l'output completo dell'encoder\n",
    "        #    Il decoder calcolerà internamente sia il contesto iniziale (ATTENZIONE #1)\n",
    "        #    sia l'attenzione per-step (ATTENZIONE #2)\n",
    "        # generated_image_256.shape: (batch_size, 3, 256, 256)\n",
    "        # generated_image_64.shape: (batch_size, 3, 64, 64)\n",
    "        generated_image_256, generated_image_64, attention_maps, initial_attention_weights = self.image_decoder(noise, encoder_output, attention_mask)\n",
    "\n",
    "        if return_attentions:\n",
    "            return generated_image_256, generated_image_64, attention_maps, initial_attention_weights\n",
    "        return generated_image_256, generated_image_64\n",
    "\n",
    "\n",
    "\n",
    "# Test the generator\n",
    "generator = Generator().to(device)\n",
    "with torch.no_grad():\n",
    "    generated_images_256, generated_images_64 = generator(\n",
    "        sample_batch['text'][:2].to(device),\n",
    "        sample_batch['attention_mask'][:2].to(device)\n",
    "    )\n",
    "print(f\"Generator output shape 256x256: {generated_images_256.shape}\")\n",
    "print(f\"Generator output shape 64x64: {generated_images_64.shape}\")\n",
    "\n",
    "# Show a sample generated image to verify it works\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(2):\n",
    "    # 256x256 images\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    img = (generated_images_256[i].cpu() + 1) / 2.0  # Denormalize\n",
    "    plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "    plt.title(f\"Generated 256x256 Sample {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # 64x64 images\n",
    "    plt.subplot(2, 2, i+3)\n",
    "    img = (generated_images_64[i].cpu() + 1) / 2.0  # Denormalize\n",
    "    plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "    plt.title(f\"Generated 64x64 Sample {i+1}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"✅ Generator test successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Discriminator for dynamic image sizes (64x64 or 256x256) with AttnGAN-style dual outputs\n",
    "class Discriminator256(nn.Module):\n",
    "    def __init__(self, text_dim=256, img_channels=3, img_size=256):\n",
    "        super(Discriminator256, self).__init__()\n",
    "\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.img_path = nn.Sequential(\n",
    "            # 256x256 -> 128x128\n",
    "            nn.Conv2d(img_channels, 16, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 128x128 -> 64x64\n",
    "            nn.Conv2d(16, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 64x64 -> 32x32\n",
    "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 32x32 -> 16x16\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 16x16 -> 8x8\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 8x8 -> 4x4\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        # Text encoder for discriminator\n",
    "        self.text_path = nn.Sequential(\n",
    "            nn.Linear(text_dim, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 512)\n",
    "        )\n",
    "\n",
    "        # AttnGAN-style dual outputs\n",
    "        # 1. Unconditional classifier (real/fake without text conditioning)\n",
    "        self.unconditional_classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # 2. Conditional classifier (text-conditioned real/fake)\n",
    "        self.conditional_classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4 + 512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, images, text_features=None, text_mask=None, return_both=True):\n",
    "        # Encode image\n",
    "        img_features = self.img_path(images)\n",
    "        img_features_flat = img_features.view(img_features.size(0), -1)  # Flatten\n",
    "\n",
    "        # Unconditional output (real/fake without text)\n",
    "        unconditional_output = self.unconditional_classifier(img_features_flat)\n",
    "\n",
    "        if not return_both:\n",
    "            return unconditional_output\n",
    "\n",
    "        # Conditional output (text-conditioned real/fake)\n",
    "        if text_features is not None and text_mask is not None:\n",
    "            # Encode text (mean pooling)\n",
    "            global_full_text = self.text_encoder(text_features, text_mask)\n",
    "            global_text = global_full_text.mean(dim=1)\n",
    "            text_features_encoded = self.text_path(global_text)\n",
    "\n",
    "            # Combine features\n",
    "            combined = torch.cat([img_features_flat, text_features_encoded], dim=1)\n",
    "            conditional_output = self.conditional_classifier(combined)\n",
    "        else:\n",
    "            # If no text provided, return zeros for conditional output\n",
    "            conditional_output = torch.zeros_like(unconditional_output)\n",
    "\n",
    "        return unconditional_output, conditional_output\n",
    "\n",
    "\n",
    "class Discriminator64(nn.Module):\n",
    "    def __init__(self, text_dim=256, img_channels=3):\n",
    "        super(Discriminator64, self).__init__()\n",
    "\n",
    "        self.text_encoder = TextEncoder()\n",
    "\n",
    "        self.img_path = nn.Sequential(\n",
    "            # 64x64 -> 32x32\n",
    "            nn.Conv2d(img_channels, 16, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 32x32 -> 16x16\n",
    "            nn.Conv2d(16, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 16x16 -> 8x8\n",
    "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 8x8 -> 4x4\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        # Text encoder for discriminator\n",
    "        self.text_path = nn.Sequential(\n",
    "            nn.Linear(text_dim, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 512)\n",
    "        )\n",
    "\n",
    "        # AttnGAN-style dual outputs\n",
    "        # 1. Unconditional classifier (real/fake without text conditioning)\n",
    "        self.unconditional_classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # 2. Conditional classifier (text-conditioned real/fake)\n",
    "        self.conditional_classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4 + 512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, images, text_features=None, text_mask=None, return_both=True):\n",
    "        # Encode image\n",
    "        img_features = self.img_path(images)\n",
    "        img_features_flat = img_features.view(img_features.size(0), -1)  # Flatten\n",
    "\n",
    "        # Unconditional output (real/fake without text)\n",
    "        unconditional_output = self.unconditional_classifier(img_features_flat)\n",
    "\n",
    "        if not return_both:\n",
    "            return unconditional_output\n",
    "\n",
    "        # Conditional output (text-conditioned real/fake)\n",
    "        if text_features is not None and text_mask is not None:\n",
    "            # Encode text (mean pooling)\n",
    "            global_full_text = self.text_encoder(text_features, text_mask)\n",
    "            global_text = global_full_text.mean(dim=1)\n",
    "            text_features_encoded = self.text_path(global_text)\n",
    "\n",
    "            # Combine features\n",
    "            combined = torch.cat([img_features_flat, text_features_encoded], dim=1)\n",
    "            conditional_output = self.conditional_classifier(combined)\n",
    "        else:\n",
    "            # If no text provided, return zeros for conditional output\n",
    "            conditional_output = torch.zeros_like(unconditional_output)\n",
    "\n",
    "        return unconditional_output, conditional_output\n",
    "\n",
    "\n",
    "# Test the discriminator with AttnGAN-style dual outputs\n",
    "discriminator_256 = Discriminator256().to(device)\n",
    "with torch.no_grad():\n",
    "    # Generate test images first\n",
    "    test_generated_images_256, test_generated_images_64 = generator(\n",
    "        sample_batch['text'][:2].to(device),\n",
    "        sample_batch['attention_mask'][:2].to(device)\n",
    "    )\n",
    "\n",
    "    # Test with 256x256 images - both outputs\n",
    "    disc_unconditional_256, disc_conditional_256 = discriminator_256(\n",
    "        test_generated_images_256,\n",
    "        sample_batch['text'][:2].to(device),\n",
    "        sample_batch['attention_mask'][:2].to(device),\n",
    "        return_both=True\n",
    "    )\n",
    "\n",
    "    # Test unconditional only\n",
    "    disc_unconditional_only = discriminator_256(\n",
    "        test_generated_images_256,\n",
    "        return_both=False\n",
    "    )\n",
    "\n",
    "print(f\"Discriminator unconditional output shape (256x256): {disc_unconditional_256.shape}\")\n",
    "print(f\"Discriminator conditional output shape (256x256): {disc_conditional_256.shape}\")\n",
    "print(f\"Discriminator unconditional-only output shape: {disc_unconditional_only.shape}\")\n",
    "print(f\"Test generated images shape (256x256): {test_generated_images_256.shape}\")\n",
    "\n",
    "# Test with 64x64 discriminator\n",
    "discriminator_64 = Discriminator64().to(device)\n",
    "with torch.no_grad():\n",
    "    # Test with 64x64 images\n",
    "    disc_unconditional_64, disc_conditional_64 = discriminator_64(\n",
    "        test_generated_images_64,\n",
    "        sample_batch['text'][:2].to(device),\n",
    "        sample_batch['attention_mask'][:2].to(device),\n",
    "        return_both=True\n",
    "    )\n",
    "\n",
    "print(f\"\\nDiscriminator unconditional output shape (64x64): {disc_unconditional_64.shape}\")\n",
    "print(f\"Discriminator conditional output shape (64x64): {disc_conditional_64.shape}\")\n",
    "print(f\"Test generated images shape (64x64): {test_generated_images_64.shape}\")\n",
    "\n",
    "# Show the architecture for both sizes\n",
    "print(f\"\\nDiscriminator architecture for 256x256:\")\n",
    "print(f\"Number of conv layers: {len([m for m in discriminator_256.img_path if isinstance(m, nn.Conv2d)])}\")\n",
    "print(f\"Final feature map size: 512 x 4 x 4\")\n",
    "print(f\"Outputs: Unconditional (real/fake) + Conditional (text-conditioned real/fake)\")\n",
    "\n",
    "print(f\"\\nDiscriminator architecture for 64x64:\")\n",
    "print(f\"Number of conv layers: {len([m for m in discriminator_64.img_path if isinstance(m, nn.Conv2d)])}\")\n",
    "print(f\"Final feature map size: 512 x 4 x 4\")\n",
    "print(f\"Outputs: Unconditional (real/fake) + Conditional (text-conditioned real/fake)\")\n",
    "\n",
    "print(\"✅ AttnGAN-style discriminator with dual outputs now supports both 64x64 and 256x256 images!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Training Setup and Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual loss using VGG19 pretrained on ImageNet.\n",
    "    We extract features at:\n",
    "      - relu1_2  (index: 3)\n",
    "      - relu2_2  (index: 8)\n",
    "      - relu3_2  (index: 17)\n",
    "      - relu4_2  (index: 26)\n",
    "    Then compute L1 distance between those feature maps.\n",
    "    Input images are in [-1,1]. We convert to [0,1], then normalize with ImageNet stats.\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        vgg19_features = models.vgg19(weights=VGG19_Weights.DEFAULT).features.to(device).eval()\n",
    "        # We only need layers up to 26 (relu4_2)\n",
    "        self.slices = nn.ModuleDict({\n",
    "            \"relu1_2\": nn.Sequential(*list(vgg19_features.children())[:4]),     # conv1_1, relu1_1, conv1_2, relu1_2\n",
    "            \"relu2_2\": nn.Sequential(*list(vgg19_features.children())[4:9]),    # pool1, conv2_1, relu2_1, conv2_2, relu2_2\n",
    "            \"relu3_2\": nn.Sequential(*list(vgg19_features.children())[9:18]),   # pool2, conv3_1, relu3_1, conv3_2, relu3_2, ...\n",
    "            \"relu4_2\": nn.Sequential(*list(vgg19_features.children())[18:27])   # pool3, conv4_1, relu4_1, conv4_2, relu4_2\n",
    "        })\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, img_gen, img_ref):\n",
    "        \"\"\"\n",
    "        img_gen, img_ref: [B,3,H,W] in range [-1,1].\n",
    "        Return: sum of L1 distances between VGG feature maps at chosen layers.\n",
    "        \"\"\"\n",
    "        # Convert to [0,1]\n",
    "        gen = (img_gen + 1.0) / 2.0\n",
    "        ref = (img_ref + 1.0) / 2.0\n",
    "        # Normalize\n",
    "        gen_norm = (gen - self.mean) / self.std\n",
    "        ref_norm = (ref - self.mean) / self.std\n",
    "\n",
    "        loss = 0.0\n",
    "        x_gen = gen_norm\n",
    "        x_ref = ref_norm\n",
    "        for slice_mod in self.slices.values():\n",
    "            x_gen = slice_mod(x_gen)\n",
    "            x_ref = slice_mod(x_ref)\n",
    "            loss += self.l1(x_gen, x_ref)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SobelLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes the Sobel loss between two images, which encourages edge similarity.\n",
    "    This loss operates on the grayscale versions of the input images.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SobelLoss, self).__init__()\n",
    "        # Sobel kernels for edge detection\n",
    "        kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        self.register_buffer(\"kernel_x\", kernel_x)\n",
    "        self.register_buffer(\"kernel_y\", kernel_y)\n",
    "        self.l1 = nn.L1Loss()\n",
    "\n",
    "        # Grayscale conversion weights (ITU-R BT.601)\n",
    "        self.register_buffer(\"rgb_to_gray_weights\", torch.tensor([0.299, 0.587, 0.114]).view(1, 3, 1, 1))\n",
    "\n",
    "    def _get_edges(self, img):\n",
    "        \"\"\"\n",
    "        Converts an RGB image to grayscale and applies Sobel filters.\n",
    "        Args:\n",
    "            img: [B, 3, H, W] image tensor in range [-1, 1].\n",
    "        Returns:\n",
    "            Gradient magnitude map [B, 1, H, W].\n",
    "        \"\"\"\n",
    "        # Ensure input is 4D\n",
    "        if img.dim() != 4:\n",
    "            raise ValueError(f\"Expected 4D input (got {img.dim()}D)\")\n",
    "\n",
    "        # Convert from [-1, 1] to [0, 1]\n",
    "        img = (img + 1.0) / 2.0\n",
    "\n",
    "        # Convert to grayscale\n",
    "        # The weights need to be on the same device as the image.\n",
    "        grayscale_img = F.conv2d(img, self.rgb_to_gray_weights.to(img.device)) # type: ignore\n",
    "\n",
    "        # Apply Sobel filters. Kernels also need to be on the correct device.\n",
    "        grad_x = F.conv2d(grayscale_img, self.kernel_x.to(img.device), padding=1) # type: ignore\n",
    "        grad_y = F.conv2d(grayscale_img, self.kernel_y.to(img.device), padding=1) # type: ignore\n",
    "\n",
    "        # Compute gradient magnitude\n",
    "        edges = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6) # add epsilon for stability\n",
    "        return edges\n",
    "\n",
    "    def forward(self, img_gen, img_ref):\n",
    "        \"\"\"\n",
    "        img_gen, img_ref: [B, 3, H, W] in range [-1, 1].\n",
    "        Returns: L1 loss between the edge maps of the two images.\n",
    "        \"\"\"\n",
    "        edges_gen = self._get_edges(img_gen)\n",
    "        edges_ref = self._get_edges(img_ref)\n",
    "        return self.l1(edges_gen, edges_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Training utilities and visualization functions from utils.py\n",
    "def weights_init(m):\n",
    "    \"\"\"Initialize model weights\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def denormalize_image(tensor):\n",
    "    \"\"\"\n",
    "    Denormalizza un tensore immagine dall'intervallo [-1, 1] a [0, 1] per la visualizzazione.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Il tensore dell'immagine, con valori in [-1, 1].\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Il tensore denormalizzato con valori in [0, 1].\n",
    "    \"\"\"\n",
    "    tensor = (tensor + 1) / 2\n",
    "    return tensor.clamp(0, 1)\n",
    "\n",
    "def save_plot_losses(losses_g, losses_d, losses_recon=None, output_dir=\"training_output\", show_inline=True):\n",
    "    \"\"\"\n",
    "    Genera e salva un plot delle loss del generatore e del discriminatore.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(losses_g, label=\"Generator Loss\", color=\"blue\")\n",
    "    ax.plot(losses_d, label=\"Discriminator Loss\", color=\"red\")\n",
    "    if losses_recon is not None:\n",
    "        ax.plot(losses_recon, label=\"Reconstruction Loss\", color=\"green\")\n",
    "    ax.set_title(\"Training Losses\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    save_path = os.path.join(output_dir, \"training_losses.png\")\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Grafico delle loss salvato in: {save_path}\")\n",
    "\n",
    "    if show_inline:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "def save_plot_non_gan_losses(train_losses_history, val_losses_history, output_dir=\"training_output\", show_inline=True, filter_losses=None):\n",
    "    \"\"\"\n",
    "    Generates and saves plots of losses for non-GAN models with multiple loss components.\n",
    "\n",
    "    Args:\n",
    "        train_losses_history: List of dicts containing training losses per epoch\n",
    "                             e.g., [{'l1': 0.5, 'sobel': 0.3, 'ssim': 0.2}, ...]\n",
    "        val_losses_history: List of dicts containing validation losses per epoch\n",
    "        output_dir: Directory to save the plot\n",
    "        show_inline: Whether to display the plot inline\n",
    "        filter_losses: Optional list of loss names to plot. If None, plots all losses.\n",
    "                      e.g., ['l1', 'sobel'] to only plot those specific losses\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if not train_losses_history or not val_losses_history:\n",
    "        print(\"No loss history to plot\")\n",
    "        return\n",
    "\n",
    "    # Extract all unique loss keys from both training and validation\n",
    "    all_keys = set()\n",
    "    for losses_dict in train_losses_history + val_losses_history:\n",
    "        all_keys.update(losses_dict.keys())\n",
    "\n",
    "    # Filter out non-numeric keys if any\n",
    "    loss_keys = [key for key in all_keys if key not in ['epoch']]\n",
    "\n",
    "    # Apply filter if specified\n",
    "    if filter_losses is not None:\n",
    "        loss_keys = [key for key in loss_keys if key in filter_losses]\n",
    "\n",
    "    loss_keys = sorted(loss_keys)  # Sort for consistent ordering\n",
    "\n",
    "    if not loss_keys:\n",
    "        print(\"No valid loss keys found\")\n",
    "        return\n",
    "\n",
    "    # Create subplots\n",
    "    n_losses = len(loss_keys)\n",
    "    cols = min(3, n_losses)  # Max 3 columns\n",
    "    rows = (n_losses + cols - 1) // cols  # Ceiling division\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows))\n",
    "    if n_losses == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    fig.suptitle(\"Training and Validation Losses\", fontsize=16, y=0.98)\n",
    "\n",
    "    for i, loss_key in enumerate(loss_keys):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        ax = axes[row, col] if rows > 1 else axes[col]\n",
    "\n",
    "        # Extract train and validation losses for this key\n",
    "        train_values = [losses.get(loss_key, 0) for losses in train_losses_history]\n",
    "        val_values = [losses.get(loss_key, 0) for losses in val_losses_history]\n",
    "\n",
    "        epochs_train = range(1, len(train_values) + 1)\n",
    "        epochs_val = range(1, len(val_values) + 1)\n",
    "\n",
    "        # Plot training and validation curves\n",
    "        if train_values:\n",
    "            ax.plot(epochs_train, train_values, label=f\"Train {loss_key}\", color=\"blue\", linewidth=1.5)\n",
    "        if val_values:\n",
    "            ax.plot(epochs_val, val_values, label=f\"Val {loss_key}\", color=\"red\", linewidth=1.5, linestyle='--')\n",
    "\n",
    "        ax.set_title(f\"{loss_key.capitalize()} Loss\", fontsize=12)\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Set y-axis to start from 0 for better visualization\n",
    "        ax.set_ylim(bottom=0)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for i in range(n_losses, rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        if rows > 1:\n",
    "            axes[row, col].set_visible(False)\n",
    "        else:\n",
    "            axes[col].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    save_path = os.path.join(output_dir, \"non_gan_training_losses.png\")\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Non-GAN training losses plot saved to: {save_path}\")\n",
    "\n",
    "    if show_inline:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def save_comparison_grid(epoch, model, batch, set_name, device, output_dir=\"training_output\", show_inline=True):\n",
    "    \"\"\"\n",
    "    Genera e salva/mostra una griglia di confronto orizzontale (reale vs. generato).\n",
    "    Enhanced version from utils.py - automatically handles 256x256 or 64x64 based on set_name\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    token_ids = batch[\"text\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    real_images = batch[\"image\"]\n",
    "    pokemon_ids = batch[\"idx\"]\n",
    "    descriptions = batch[\"description\"]\n",
    "    num_images = real_images.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_images = model(token_ids, attention_mask)\n",
    "        # Handle the case where generator returns both 256x256 and 64x64 images\n",
    "        if isinstance(generated_images, tuple):\n",
    "            # Check if we want 64x64 or 256x256 based on set_name\n",
    "            if \"64\" in set_name:\n",
    "                generated_images = generated_images[1]  # Use 64x64 output\n",
    "                # Resize real images to 64x64 for comparison\n",
    "                real_images = F.interpolate(real_images, size=(64, 64), mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                generated_images = generated_images[0]  # Use 256x256 output\n",
    "\n",
    "    fig, axs = plt.subplots(2, num_images, figsize=(4 * num_images, 8.5))\n",
    "    resolution = \"64x64\" if \"64\" in set_name else \"256x256\"\n",
    "    fig.suptitle(\n",
    "        f\"Epoch {epoch} - {set_name.capitalize()} Comparison ({resolution})\", fontsize=16, y=0.98\n",
    "    )\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Riga 0: Immagini Reali\n",
    "        ax_real = axs[0, i]\n",
    "        ax_real.imshow(denormalize_image(real_images[i].cpu()).permute(1, 2, 0))\n",
    "        ax_real.set_title(f\"#{pokemon_ids[i]}: {descriptions[i][:35]}...\", fontsize=10)\n",
    "        ax_real.axis(\"off\")\n",
    "\n",
    "        # Riga 1: Immagini Generate\n",
    "        ax_gen = axs[1, i]\n",
    "        ax_gen.imshow(denormalize_image(generated_images[i].cpu()).permute(1, 2, 0))\n",
    "        ax_gen.axis(\"off\")\n",
    "\n",
    "    axs[0, 0].text(\n",
    "        -0.1,\n",
    "        0.5,\n",
    "        \"Real\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        rotation=\"vertical\",\n",
    "        fontsize=14,\n",
    "        transform=axs[0, 0].transAxes,\n",
    "    )\n",
    "    axs[1, 0].text(\n",
    "        -0.1,\n",
    "        0.5,\n",
    "        \"Generated\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        rotation=\"vertical\",\n",
    "        fontsize=14,\n",
    "        transform=axs[1, 0].transAxes,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "\n",
    "    # Salva sempre l'immagine\n",
    "    save_path = os.path.join(output_dir, f\"{epoch:03d}_{set_name}_comparison.png\")\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    if show_inline:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "def save_attention_visualization(epoch, model, tokenizer, batch, device, set_name, output_dir=\"training_output\", show_inline=True):\n",
    "    \"\"\"\n",
    "    Genera e salva una visualizzazione dell'attenzione multi-livello in stile griglia.\n",
    "    Enhanced version from utils.py\n",
    "\n",
    "    L'immagine mostra:\n",
    "    1. In alto, l'immagine generata e il prompt.\n",
    "    2. Sotto, un bar chart dell'attenzione iniziale (contesto globale).\n",
    "    3. Di seguito, una serie di griglie, una per ogni strato di attenzione del decoder.\n",
    "       Ciascuna griglia mostra le mappe di calore pure per ogni token rilevante.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_ids = batch[\"text\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        if token_ids.dim() > 1:  # Assicura un batch di 1\n",
    "            token_ids = token_ids[0].unsqueeze(0)\n",
    "            attention_mask = attention_mask[0].unsqueeze(0)\n",
    "\n",
    "        pokemon_id = batch[\"idx\"][0]\n",
    "        description = batch[\"description\"][0]\n",
    "\n",
    "        model_to_use = model.module if isinstance(model, nn.DataParallel) else model\n",
    "        generated_image, _, attention_maps, initial_context_weights = model_to_use(\n",
    "            token_ids, attention_mask, return_attentions=True\n",
    "        )\n",
    "\n",
    "    decoder_attention_maps = [m for m in attention_maps if m is not None]\n",
    "\n",
    "    if not decoder_attention_maps or initial_context_weights is None:\n",
    "        print(\n",
    "            f\"Epoch {epoch}: Mappe di attenzione non disponibili. Salto la visualizzazione.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    tokens_all = tokenizer.convert_ids_to_tokens(token_ids.squeeze(0))\n",
    "    display_tokens = []\n",
    "    for i, token in enumerate(tokens_all):\n",
    "        if (\n",
    "            token not in [tokenizer.sep_token, tokenizer.pad_token]\n",
    "            and attention_mask[0, i] == 1\n",
    "        ):\n",
    "            display_tokens.append({\"token\": token, \"index\": i})\n",
    "\n",
    "    if not display_tokens:\n",
    "        print(\n",
    "            f\"Epoch {epoch}: Nessun token valido da visualizzare per '{description}'. Salto.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    token_indices_to_display = [t[\"index\"] for t in display_tokens]\n",
    "    img_tensor_cpu = denormalize_image(generated_image.squeeze(0).cpu()).permute(\n",
    "        1, 2, 0\n",
    "    )\n",
    "    num_decoder_layers = len(decoder_attention_maps)\n",
    "    num_tokens = len(display_tokens)\n",
    "\n",
    "    # --- Creazione del Plot ---\n",
    "    # Calcola dinamicamente layout e dimensioni\n",
    "    cols = min(num_tokens, 8)\n",
    "    rows_per_layer = (num_tokens + cols - 1) // cols\n",
    "    num_main_rows = (\n",
    "        2 + num_decoder_layers\n",
    "    )  # Immagine, Bar chart, e N layer di attenzione\n",
    "    # Altezza per immagine, bar chart, e poi per ogni riga di ogni layer\n",
    "    height_ratios = [3, 2] + [2 * rows_per_layer] * num_decoder_layers\n",
    "    fig_height = sum(height_ratios)\n",
    "    fig_width = max(20, 2.5 * cols)\n",
    "\n",
    "    fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "    gs_main = fig.add_gridspec(\n",
    "        num_main_rows, 1, height_ratios=height_ratios, hspace=1.2\n",
    "    )\n",
    "    fig.suptitle(\n",
    "        f\"Epoch {epoch}: Attention Visualization for Pokémon #{pokemon_id} ({set_name.capitalize()})\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "\n",
    "    # --- 1. Immagine Generata e Prompt ---\n",
    "    ax_main_img = fig.add_subplot(gs_main[0])\n",
    "    ax_main_img.imshow(img_tensor_cpu)\n",
    "    ax_main_img.set_title(\"Generated Image\", fontsize=18)\n",
    "    ax_main_img.text(\n",
    "        0.5,\n",
    "        -0.1,\n",
    "        f\"Prompt: {description}\",\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "        transform=ax_main_img.transAxes,\n",
    "        fontsize=14,\n",
    "        wrap=True,\n",
    "    )\n",
    "    ax_main_img.axis(\"off\")\n",
    "\n",
    "    # --- 2. Attenzione Iniziale per il Contesto (bar chart) ---\n",
    "    ax_initial_attn = fig.add_subplot(gs_main[1])\n",
    "    initial_weights_squeezed = initial_context_weights.squeeze().cpu().numpy()\n",
    "    token_strings = [t[\"token\"] for t in display_tokens]\n",
    "    token_indices = [t[\"index\"] for t in display_tokens]\n",
    "    relevant_weights = initial_weights_squeezed[token_indices]\n",
    "    ax_initial_attn.bar(\n",
    "        np.arange(len(token_strings)), relevant_weights, color=\"skyblue\"\n",
    "    )\n",
    "    ax_initial_attn.set_xticks(np.arange(len(token_strings)))\n",
    "    ax_initial_attn.set_xticklabels(token_strings, rotation=45, ha=\"right\", fontsize=10)\n",
    "    ax_initial_attn.set_title(\"Initial Context Attention (Global)\", fontsize=16)\n",
    "    ax_initial_attn.set_ylabel(\"Weight\", fontsize=12)\n",
    "    ax_initial_attn.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # --- 3. Attenzione per Strato del Decoder (griglie di heatmap) ---\n",
    "    for i, layer_attn_map in enumerate(decoder_attention_maps):\n",
    "        map_size_flat = layer_attn_map.shape[1]\n",
    "        map_side = int(np.sqrt(map_size_flat))\n",
    "        layer_title = (\n",
    "            f\"Decoder Cross-Attention Layer {i + 1} (Size: {map_side}x{map_side})\"\n",
    "        )\n",
    "        layer_attn_map_squeezed = layer_attn_map.squeeze(0).cpu()\n",
    "\n",
    "        # Seleziona solo le mappe di attenzione per i token che visualizziamo\n",
    "        relevant_attn_maps = layer_attn_map_squeezed[:, token_indices_to_display]\n",
    "\n",
    "        # Trova i valori min/max per questo strato per la colorbar\n",
    "        vmin = relevant_attn_maps.min()\n",
    "        vmax = relevant_attn_maps.max()\n",
    "\n",
    "        # Crea una subgrid per questo strato (con una colonna in più per la colorbar)\n",
    "        gs_layer = gs_main[2 + i].subgridspec(\n",
    "            rows_per_layer,\n",
    "            cols + 1,\n",
    "            wspace=0.2,\n",
    "            hspace=0.4,\n",
    "            width_ratios=[*([1] * cols), 0.1],\n",
    "        )\n",
    "\n",
    "        # Crea tutti gli assi per la griglia\n",
    "        axes_in_layer = [\n",
    "            fig.add_subplot(gs_layer[r, c])\n",
    "            for r in range(rows_per_layer)\n",
    "            for c in range(cols)\n",
    "        ]\n",
    "\n",
    "        # Usa la posizione del primo asse per il titolo\n",
    "        if axes_in_layer:\n",
    "            y_pos = axes_in_layer[0].get_position().y1\n",
    "            fig.text(\n",
    "                0.5,\n",
    "                y_pos + 0.01,\n",
    "                layer_title,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=16,\n",
    "                weight=\"bold\",\n",
    "            )\n",
    "\n",
    "        for j, token_info in enumerate(display_tokens):\n",
    "            ax = axes_in_layer[j]\n",
    "            attn_for_token = layer_attn_map_squeezed[:, token_info[\"index\"]]\n",
    "            heatmap = attn_for_token.reshape(map_side, map_side)\n",
    "            im = ax.imshow(\n",
    "                heatmap, cmap=\"jet\", interpolation=\"nearest\", vmin=vmin, vmax=vmax\n",
    "            )\n",
    "            ax.set_title(f\"'{token_info['token']}'\", fontsize=12)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        # Aggiungi la colorbar\n",
    "        cax = fig.add_subplot(gs_layer[:, -1])\n",
    "        cbar = fig.colorbar(im, cax=cax)\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "        cbar.set_label(\"Attention Weight\", rotation=270, labelpad=15, fontsize=12)\n",
    "\n",
    "        # Pulisce gli assi non usati nella griglia\n",
    "        for j in range(num_tokens, len(axes_in_layer)):\n",
    "            axes_in_layer[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0.03, 1, 0.96))\n",
    "    save_path = os.path.join(\n",
    "        output_dir, f\"{epoch:03d}_{set_name}_attention_visualization.png\"\n",
    "    )\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "\n",
    "    if show_inline:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "def save_checkpoint(generator, discriminator, g_optimizer, d_optimizer, epoch, losses, path):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "        'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "        'losses': losses\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize models with enhanced discriminator\n",
    "generator = Generator().to(device)\n",
    "discriminator_256 = Discriminator256(img_size=256).to(device)  # 256x256 discriminator\n",
    "discriminator_64 = Discriminator64().to(device)    # 64x64 discriminator\n",
    "\n",
    "# Apply weight initialization\n",
    "generator.apply(weights_init)\n",
    "discriminator_256.apply(weights_init)\n",
    "discriminator_64.apply(weights_init)\n",
    "\n",
    "print(\"✅ Enhanced discriminators initialized with dynamic layer calculation!\")\n",
    "\n",
    "# Setup optimizers for AttnGAN-style training\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_D_256 = optim.Adam(discriminator_256.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_D_64 = optim.Adam(discriminator_64.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Loss function\n",
    "adv_criterion = nn.BCELoss().to(device)\n",
    "l1_criterion = nn.L1Loss().to(device)\n",
    "perc_criterion = VGGPerceptualLoss(device)\n",
    "sobel_criterion = SobelLoss().to(device)\n",
    "\n",
    "print(\"Models and optimizers initialized successfully!\")\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Discriminator 256 parameters: {sum(p.numel() for p in discriminator_256.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Discriminator 64 parameters: {sum(p.numel() for p in discriminator_64.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in generator.parameters() if p.requires_grad) + sum(p.numel() for p in discriminator_256.parameters() if p.requires_grad) + sum(p.numel() for p in discriminator_64.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. GAN Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "losses = {\n",
    "    'generator': [],\n",
    "    'discriminator': [],\n",
    "    'l1': [],\n",
    "    'perceptual': [],\n",
    "    'sobel': [],\n",
    "}\n",
    "\n",
    "# Validation history (separate tracking)\n",
    "val_losses = {\n",
    "    'l1': [],\n",
    "    'perceptual': [],\n",
    "    'sobel': [],\n",
    "    'total': [],\n",
    "}\n",
    "\n",
    "def validate_model(generator, val_loader, device, l1_criterion, perc_criterion, sobel_criterion):\n",
    "    \"\"\"\n",
    "    Validate the model on the validation set\n",
    "    Returns validation losses (dict)\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "\n",
    "    val_l1_loss = 0.0\n",
    "    val_perc_loss = 0.0\n",
    "    val_sobel_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move data to device\n",
    "            real_images = batch['image'].to(device)\n",
    "            text_ids = batch['text'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Generate images\n",
    "            generated_images, _ = generator(text_ids, attention_mask)\n",
    "\n",
    "            # Calculate validation losses (no adversarial loss)\n",
    "            batch_l1_loss = l1_criterion(generated_images, real_images)\n",
    "            batch_perc_loss = perc_criterion(generated_images, real_images)\n",
    "            batch_sobel_loss = sobel_criterion(generated_images, real_images)\n",
    "\n",
    "            val_l1_loss += batch_l1_loss.item()\n",
    "            val_perc_loss += batch_perc_loss.item()\n",
    "            val_sobel_loss += batch_sobel_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_val_l1 = val_l1_loss / num_batches\n",
    "    avg_val_perc = val_perc_loss / num_batches\n",
    "    avg_val_sobel = val_sobel_loss / num_batches\n",
    "    avg_val_total = avg_val_l1 + avg_val_perc + avg_val_sobel\n",
    "\n",
    "    # Set models back to training mode\n",
    "    generator.train()\n",
    "\n",
    "    return {\n",
    "        'l1': avg_val_l1,\n",
    "        'perceptual': avg_val_perc,\n",
    "        'sobel': avg_val_sobel,\n",
    "        'total': avg_val_total\n",
    "    }\n",
    "\n",
    "epoch = 0\n",
    "noise_dim = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Training parameters\n",
    "total_epochs = 150 # Reduced for faster training in demo\n",
    "display_interval = 1\n",
    "save_interval = 15\n",
    "clear_interval = 22\n",
    "\n",
    "# AttnGAN-style training - no generator update intervals\n",
    "lambda_l1 = 1.0\n",
    "lambda_adv = 1.0\n",
    "lambda_perceptual = 0.0\n",
    "lambda_sobel = 0.0\n",
    "\n",
    "# Labels for real and fake data\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "\n",
    "def create_mismatched_text_batch(text_ids, attention_mask):\n",
    "    \"\"\"Create a batch with mismatched text for wrong text conditioning\"\"\"\n",
    "    batch_size = text_ids.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    return text_ids[indices], attention_mask[indices]\n",
    "\n",
    "def compute_discriminator_loss(discriminator, real_images, fake_images,\n",
    "                              text_ids, attention_mask, wrong_text_ids, wrong_attention_mask,\n",
    "                              real_labels, fake_labels, adv_criterion):\n",
    "    \"\"\"Compute AttnGAN-style discriminator loss with 5 components\"\"\"\n",
    "    # Real images with correct text\n",
    "    real_uncond, real_cond = discriminator(real_images, text_ids, attention_mask, return_both=True)\n",
    "    real_uncond_loss = adv_criterion(real_uncond, real_labels)\n",
    "    real_cond_loss = adv_criterion(real_cond, real_labels)\n",
    "\n",
    "    # Real images with wrong text\n",
    "    _, real_cond_wrong = discriminator(real_images, wrong_text_ids, wrong_attention_mask, return_both=True)\n",
    "    real_cond_wrong_loss = adv_criterion(real_cond_wrong, fake_labels)\n",
    "\n",
    "    # Fake images with correct text\n",
    "    fake_uncond, fake_cond = discriminator(fake_images.detach(), text_ids, attention_mask, return_both=True)\n",
    "    fake_uncond_loss = adv_criterion(fake_uncond, fake_labels)\n",
    "    fake_cond_loss = adv_criterion(fake_cond, fake_labels)\n",
    "\n",
    "    # Average all 5 losses\n",
    "    total_loss = (real_uncond_loss + real_cond_loss + real_cond_wrong_loss +\n",
    "                  fake_uncond_loss + fake_cond_loss) / 5\n",
    "\n",
    "    # Return both total loss and components for tracking\n",
    "    components = {\n",
    "        'real_uncond': real_uncond_loss.item(),\n",
    "        'real_cond': real_cond_loss.item(),\n",
    "        'real_wrong': real_cond_wrong_loss.item(),\n",
    "        'fake_uncond': fake_uncond_loss.item(),\n",
    "        'fake_cond': fake_cond_loss.item()\n",
    "    }\n",
    "\n",
    "    return total_loss, components\n",
    "\n",
    "def compute_generator_adversarial_loss(discriminator, fake_images, text_ids, attention_mask,\n",
    "                                     real_labels, adv_criterion):\n",
    "    \"\"\"Compute generator adversarial loss for one discriminator\"\"\"\n",
    "    fake_uncond, fake_cond = discriminator(fake_images, text_ids, attention_mask, return_both=True)\n",
    "    uncond_loss = adv_criterion(fake_uncond, real_labels)\n",
    "    cond_loss = adv_criterion(fake_cond, real_labels)\n",
    "    return (uncond_loss + cond_loss) / 2\n",
    "\n",
    "def compute_reconstruction_losses(fake_images_256, real_images,\n",
    "                                l1_criterion, perc_criterion, sobel_criterion,\n",
    "                                lambda_l1, lambda_perceptual, lambda_sobel, device):\n",
    "    \"\"\"Compute all reconstruction losses\"\"\"\n",
    "    l1_loss = l1_criterion(fake_images_256, real_images) if lambda_l1 > 0 else torch.tensor(0.0, device=device)\n",
    "    perc_loss = perc_criterion(fake_images_256, real_images) if lambda_perceptual > 0 else torch.tensor(0.0, device=device)\n",
    "    sobel_loss = sobel_criterion(fake_images_256, real_images) if lambda_sobel > 0 else torch.tensor(0.0, device=device)\n",
    "    return l1_loss, perc_loss, sobel_loss\n",
    "\n",
    "print(\"Starting AttnGAN-style training with dual discriminators...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {dataloader.batch_size}\")\n",
    "print(f\"Total epochs: {total_epochs}\")\n",
    "print(f\"Using dual discriminators: 64x64 and 256x256\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(epoch, total_epochs):\n",
    "    epoch_g_loss = 0.0\n",
    "    epoch_d_loss_64 = 0.0\n",
    "    epoch_d_loss_256 = 0.0\n",
    "    epoch_l1_loss = 0.0\n",
    "    epoch_perc_loss = 0.0\n",
    "    epoch_sobel_loss = 0.0\n",
    "\n",
    "    # Track discriminator loss components\n",
    "    epoch_d256_components = {'real_uncond': 0.0, 'real_cond': 0.0, 'real_wrong': 0.0, 'fake_uncond': 0.0, 'fake_cond': 0.0}\n",
    "    epoch_d64_components = {'real_uncond': 0.0, 'real_cond': 0.0, 'real_wrong': 0.0, 'fake_uncond': 0.0, 'fake_cond': 0.0}\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        batch_size = batch['image'].size(0)\n",
    "\n",
    "        # Move data to device\n",
    "        real_images = batch['image'].to(device)\n",
    "        text_ids = batch['text'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Create mismatched text and labels\n",
    "        wrong_text_ids, wrong_attention_mask = create_mismatched_text_batch(text_ids, attention_mask)\n",
    "        real_labels = torch.full((batch_size, 1), real_label, device=device, dtype=torch.float)\n",
    "        fake_labels = torch.full((batch_size, 1), fake_label, device=device, dtype=torch.float)\n",
    "\n",
    "        # Generate fake images\n",
    "        fake_images_256, fake_images_64 = generator(text_ids, attention_mask)\n",
    "        real_images_64 = F.interpolate(real_images, size=(64, 64), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # ==========================================\n",
    "        # Train Both Discriminators\n",
    "        # ==========================================\n",
    "        optimizer_D_256.zero_grad()\n",
    "\n",
    "        # 256x256 discriminator loss\n",
    "        d_loss_256, d256_components = compute_discriminator_loss(\n",
    "            discriminator_256, real_images, fake_images_256,\n",
    "            text_ids, attention_mask, wrong_text_ids, wrong_attention_mask,\n",
    "            real_labels, fake_labels, adv_criterion\n",
    "        )\n",
    "        d_loss_256.backward()\n",
    "\n",
    "        # 64x64 discriminator loss\n",
    "        d_loss_64, d64_components = compute_discriminator_loss(\n",
    "            discriminator_64, real_images_64, fake_images_64,\n",
    "            text_ids, attention_mask, wrong_text_ids, wrong_attention_mask,\n",
    "            real_labels, fake_labels, adv_criterion\n",
    "        )\n",
    "        d_loss_64.backward()\n",
    "\n",
    "        # Update discriminators\n",
    "        optimizer_D_256.step()\n",
    "        optimizer_D_64.step()\n",
    "\n",
    "        # ==========================================\n",
    "        # Train Generator\n",
    "        # ==========================================\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Adversarial losses for both discriminators\n",
    "        g_adv_loss_256 = compute_generator_adversarial_loss(\n",
    "            discriminator_256, fake_images_256, text_ids, attention_mask, real_labels, adv_criterion\n",
    "        )\n",
    "        g_adv_loss_64 = compute_generator_adversarial_loss(\n",
    "            discriminator_64, fake_images_64, text_ids, attention_mask, real_labels, adv_criterion\n",
    "        )\n",
    "        adversarial_loss = (g_adv_loss_256 + g_adv_loss_64) / 2\n",
    "\n",
    "        # Reconstruction losses\n",
    "        l1_loss, perc_loss, sobel_loss = compute_reconstruction_losses(\n",
    "            fake_images_256, real_images, l1_criterion, perc_criterion, sobel_criterion,\n",
    "            lambda_l1, lambda_perceptual, lambda_sobel, device\n",
    "        )\n",
    "\n",
    "        # Total generator loss\n",
    "        g_loss = (lambda_adv * adversarial_loss + lambda_l1 * l1_loss +\n",
    "                 lambda_perceptual * perc_loss + lambda_sobel * sobel_loss)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Update loss tracking\n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_d_loss_256 += d_loss_256.item()\n",
    "        epoch_d_loss_64 += d_loss_64.item()\n",
    "        epoch_l1_loss += l1_loss.item()\n",
    "        epoch_perc_loss += perc_loss.item()\n",
    "        epoch_sobel_loss += sobel_loss.item()\n",
    "\n",
    "        # Update discriminator component tracking\n",
    "        for key in epoch_d256_components:\n",
    "            epoch_d256_components[key] += d256_components[key]\n",
    "            epoch_d64_components[key] += d64_components[key]\n",
    "\n",
    "        # Update progress bar with detailed discriminator components\n",
    "        progress_bar.set_postfix({\n",
    "            'D256': f'{d_loss_256.item():.3f}',\n",
    "            'D256_r_u': f'{d256_components[\"real_uncond\"]:.3f}',\n",
    "            'D256_r_c': f'{d256_components[\"real_cond\"]:.3f}',\n",
    "            'D256_rw': f'{d256_components[\"real_wrong\"]:.3f}',\n",
    "            'D256_f_u': f'{d256_components[\"fake_uncond\"]:.3f}',\n",
    "            'D256_f_c': f'{d256_components[\"fake_cond\"]:.3f}',\n",
    "            'D64': f'{d_loss_64.item():.3f}',\n",
    "            'D64_r_u': f'{d64_components[\"real_uncond\"]:.3f}',\n",
    "            'D64_r_c': f'{d64_components[\"real_cond\"]:.3f}',\n",
    "            'D64_rw': f'{d64_components[\"real_wrong\"]:.3f}',\n",
    "            'D64_f_u': f'{d64_components[\"fake_uncond\"]:.3f}',\n",
    "            'D64_f_c': f'{d64_components[\"fake_cond\"]:.3f}',\n",
    "            'G': f'{g_loss.item():.3f}',\n",
    "            'L1': f'{l1_loss.item():.3f}',\n",
    "            'Adv': f'{adversarial_loss.item():.3f}'\n",
    "        })\n",
    "\n",
    "    # Calculate average losses for the epoch\n",
    "    avg_g_loss = epoch_g_loss / len(dataloader)\n",
    "    avg_d_loss_256 = epoch_d_loss_256 / len(dataloader)\n",
    "    avg_d_loss_64 = epoch_d_loss_64 / len(dataloader)\n",
    "    avg_l1_loss = epoch_l1_loss / len(dataloader)\n",
    "    avg_perc_loss = epoch_perc_loss / len(dataloader)\n",
    "    avg_sobel_loss = epoch_sobel_loss / len(dataloader)\n",
    "\n",
    "    # Calculate average discriminator components\n",
    "    avg_d256_components = {key: val / len(dataloader) for key, val in epoch_d256_components.items()}\n",
    "    avg_d64_components = {key: val / len(dataloader) for key, val in epoch_d64_components.items()}\n",
    "\n",
    "    # Store losses (combine discriminator losses)\n",
    "    losses['generator'].append(avg_g_loss)\n",
    "    losses['discriminator'].append((avg_d_loss_256 + avg_d_loss_64) / 2)\n",
    "    losses['l1'].append(avg_l1_loss)\n",
    "    losses['perceptual'].append(avg_perc_loss)\n",
    "    losses['sobel'].append(avg_sobel_loss)\n",
    "\n",
    "    # Run validation\n",
    "    print(f\"Running validation for epoch {epoch+1}...\")\n",
    "    validation_results = validate_model(generator, val_loader, device,\n",
    "                                      l1_criterion, perc_criterion, sobel_criterion)\n",
    "\n",
    "    # Store validation losses\n",
    "    val_losses['l1'].append(validation_results['l1'])\n",
    "    val_losses['perceptual'].append(validation_results['perceptual'])\n",
    "    val_losses['sobel'].append(validation_results['sobel'])\n",
    "    val_losses['total'].append(validation_results['total'])\n",
    "\n",
    "    if (epoch + 1) % clear_interval == 0:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}]\")\n",
    "    print(f\"  Train - D_256: {avg_d_loss_256:.4f}, D_64: {avg_d_loss_64:.4f}, G_loss: {avg_g_loss:.4f}\")\n",
    "    print(f\"  D_256 Components - RU: {avg_d256_components['real_uncond']:.4f}, RC: {avg_d256_components['real_cond']:.4f}, RW: {avg_d256_components['real_wrong']:.4f}, FU: {avg_d256_components['fake_uncond']:.4f}, FC: {avg_d256_components['fake_cond']:.4f}\")\n",
    "    print(f\"  D_64 Components  - RU: {avg_d64_components['real_uncond']:.4f}, RC: {avg_d64_components['real_cond']:.4f}, RW: {avg_d64_components['real_wrong']:.4f}, FU: {avg_d64_components['fake_uncond']:.4f}, FC: {avg_d64_components['fake_cond']:.4f}\")\n",
    "    print(f\"  Train - L1: {avg_l1_loss:.4f}, Perceptual: {avg_perc_loss:.4f}, Sobel: {avg_sobel_loss:.4f}\")\n",
    "    print(f\"  Val   - L1: {validation_results['l1']:.4f}, Perceptual: {validation_results['perceptual']:.4f}, Sobel: {validation_results['sobel']:.4f}, Total: {validation_results['total']:.4f}\")\n",
    "    print(f\"  Legend: RU=RealUncond, RC=RealCond, RW=RealWrong, FU=FakeUncond, FC=FakeCond\")\n",
    "\n",
    "    # Display generated images\n",
    "    if (epoch + 1) % display_interval == 0:\n",
    "        print(f\"\\nGenerating sample images at epoch {epoch+1}:\")\n",
    "        print(\"256x256 Training Images:\")\n",
    "        save_comparison_grid(epoch+1, generator, fixed_train_batch, \"train_256\", device, show_inline=True)\n",
    "        print(\"64x64 Training Images:\")\n",
    "        save_comparison_grid(epoch+1, generator, fixed_train_batch, \"train_64\", device, show_inline=True)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint_path = f'models/checkpoint_epoch_{epoch+1}.pth'\n",
    "        all_losses = {'train': losses, 'val': val_losses}\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_256_state_dict': discriminator_256.state_dict(),\n",
    "            'discriminator_64_state_dict': discriminator_64.state_dict(),\n",
    "            'g_optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'd_optimizer_state_dict': optimizer_D_256.state_dict(),\n",
    "            'd_64_optimizer_state_dict': optimizer_D_64.state_dict(),\n",
    "            'losses': all_losses\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        print(\"256x256 Validation Images:\")\n",
    "        save_comparison_grid(epoch+1, generator, fixed_val_batch, \"val_256\", device, show_inline=True)\n",
    "        print(\"64x64 Validation Images:\")\n",
    "        save_comparison_grid(epoch+1, generator, fixed_val_batch, \"val_64\", device, show_inline=True)\n",
    "        save_attention_visualization(epoch+1, generator, tokenizer, fixed_train_batch, device, \"train\", show_inline=True)\n",
    "        save_attention_visualization(epoch+1, generator, tokenizer, fixed_val_batch, device, \"val\", show_inline=True)\n",
    "\n",
    "print(\"\\nAttnGAN-style training completed!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Training Analysis and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced plot training losses using utils.py function\n",
    "save_plot_losses(\n",
    "    losses_g=losses['generator'],\n",
    "    losses_d=losses['discriminator'],\n",
    "    output_dir=\"training_output\",\n",
    "    show_inline=True\n",
    ")\n",
    "\n",
    "# Plot training vs validation losses for non-adversarial components\n",
    "# Convert to list of dicts format expected by save_plot_non_gan_losses\n",
    "train_losses_history = []\n",
    "val_losses_history = []\n",
    "\n",
    "for i in range(len(losses['l1'])):\n",
    "    train_losses_history.append({\n",
    "        'l1': losses['l1'][i],\n",
    "        'perceptual': losses['perceptual'][i],\n",
    "        'sobel': losses['sobel'][i],\n",
    "        'total': losses['l1'][i] + losses['perceptual'][i] + losses['sobel'][i]\n",
    "    })\n",
    "\n",
    "for i in range(len(val_losses['l1'])):\n",
    "    val_losses_history.append({\n",
    "        'l1': val_losses['l1'][i],\n",
    "        'perceptual': val_losses['perceptual'][i],\n",
    "        'sobel': val_losses['sobel'][i],\n",
    "        'total': val_losses['total'][i]\n",
    "    })\n",
    "\n",
    "save_plot_non_gan_losses(\n",
    "    train_losses_history=train_losses_history,\n",
    "    val_losses_history=val_losses_history,\n",
    "    output_dir=\"training_output\",\n",
    "    show_inline=True\n",
    ")\n",
    "\n",
    "# Print final statistics\n",
    "if losses['generator']:\n",
    "    print(f\"Final Train - Generator Loss: {losses['generator'][-1]:.4f}\")\n",
    "    print(f\"Final Train - Discriminator Loss: {losses['discriminator'][-1]:.4f}\")\n",
    "    print(f\"Final Train - L1 Loss: {losses['l1'][-1]:.4f}\")\n",
    "    print(f\"Final Train - Perceptual Loss: {losses['perceptual'][-1]:.4f}\")\n",
    "    print(f\"Final Train - Sobel Loss: {losses['sobel'][-1]:.4f}\")\n",
    "\n",
    "    if val_losses['l1']:\n",
    "        print(f\"Final Val   - L1 Loss: {val_losses['l1'][-1]:.4f}\")\n",
    "        print(f\"Final Val   - Perceptual Loss: {val_losses['perceptual'][-1]:.4f}\")\n",
    "        print(f\"Final Val   - Sobel Loss: {val_losses['sobel'][-1]:.4f}\")\n",
    "        print(f\"Final Val   - Total Loss: {val_losses['total'][-1]:.4f}\")\n",
    "else:\n",
    "    print(\"No training losses recorded yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of final results\n",
    "print(\"Final Results - Generated Pokemon Sprites (256x256):\")\n",
    "batch = next(iter(dataloader))\n",
    "save_comparison_grid(0, generator, batch, \"final_256\", device, show_inline=True)\n",
    "\n",
    "print(\"Final Results - Generated Pokemon Sprites (64x64):\")\n",
    "save_comparison_grid(0, generator, batch, \"final_64\", device, show_inline=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Interactive Demo with Custom Text Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced interactive generation function with attention visualization\n",
    "def generate_pokemon_from_text(description, num_samples=4, show_attention=False):\n",
    "    \"\"\"Generate Pokemon sprites from custom text description with enhanced visualization\"\"\"\n",
    "    generator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the description\n",
    "        tokens = tokenizer(\n",
    "            description,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Repeat for multiple samples\n",
    "        text_ids = tokens['input_ids'].repeat(num_samples, 1).to(device)\n",
    "        attention_mask = tokens['attention_mask'].repeat(num_samples, 1).to(device)\n",
    "\n",
    "        # Generate images (generator handles text encoding internally)\n",
    "        if show_attention:\n",
    "            generated_images_256, generated_images_64, attention_maps, initial_weights = generator(\n",
    "                text_ids, attention_mask, return_attentions=True\n",
    "            )\n",
    "        else:\n",
    "            generated_images_256, generated_images_64 = generator(text_ids, attention_mask)\n",
    "\n",
    "        # Create batch format for visualization functions - 256x256\n",
    "        fake_batch_256 = {\n",
    "            'text': text_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'image': generated_images_256,  # Use generated as \"real\" for display\n",
    "            'description': [description] * num_samples,\n",
    "            'pokemon_name': [f\"Generated_{i+1}\" for i in range(num_samples)],\n",
    "            'idx': list(range(num_samples))\n",
    "        }\n",
    "\n",
    "        # Create batch format for visualization functions - 64x64\n",
    "        fake_batch_64 = {\n",
    "            'text': text_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'image': generated_images_64,  # Use generated as \"real\" for display\n",
    "            'description': [description] * num_samples,\n",
    "            'pokemon_name': [f\"Generated_{i+1}\" for i in range(num_samples)],\n",
    "            'idx': list(range(num_samples))\n",
    "        }\n",
    "\n",
    "        # Use enhanced comparison grid for both resolutions\n",
    "        print(\"256x256 Generated Pokemon:\")\n",
    "        save_comparison_grid(\n",
    "            epoch=0,\n",
    "            model=generator,\n",
    "            batch=fake_batch_256,\n",
    "            set_name=\"custom_256\",\n",
    "            device=device,\n",
    "            output_dir=\"custom_generation\",\n",
    "            show_inline=True\n",
    "        )\n",
    "\n",
    "        print(\"64x64 Generated Pokemon:\")\n",
    "        save_comparison_grid(\n",
    "            epoch=0,\n",
    "            model=generator,\n",
    "            batch=fake_batch_64,\n",
    "            set_name=\"custom_64\",\n",
    "            device=device,\n",
    "            output_dir=\"custom_generation\",\n",
    "            show_inline=True\n",
    "        )\n",
    "\n",
    "        # Show attention visualization if requested\n",
    "        if show_attention and attention_maps is not None:\n",
    "            print(\"\\nGenerating attention visualization...\")\n",
    "            # Create single-sample batch for attention visualization\n",
    "            single_batch = {\n",
    "                'text': text_ids[:1],\n",
    "                'attention_mask': attention_mask[:1],\n",
    "                'description': [description],\n",
    "                'idx': [0]\n",
    "            }\n",
    "            save_attention_visualization(\n",
    "                epoch=0,\n",
    "                model=generator,\n",
    "                tokenizer=tokenizer,\n",
    "                batch=single_batch,\n",
    "                device=device,\n",
    "                set_name=\"custom\",\n",
    "                output_dir=\"custom_generation\",\n",
    "                show_inline=True\n",
    "            )\n",
    "\n",
    "    generator.train()\n",
    "\n",
    "# Simple visualization function for basic usage\n",
    "def simple_generate_pokemon(description, num_samples=4):\n",
    "    \"\"\"Simple generation without attention - for quick testing\"\"\"\n",
    "    generator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer(\n",
    "            description,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        text_ids = tokens['input_ids'].repeat(num_samples, 1).to(device)\n",
    "        attention_mask = tokens['attention_mask'].repeat(num_samples, 1).to(device)\n",
    "        generated_images_256, generated_images_64 = generator(text_ids, attention_mask)\n",
    "\n",
    "        # Simple matplotlib visualization for both resolutions\n",
    "        fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 3, 6))\n",
    "        if num_samples == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # 256x256 images\n",
    "            img_256 = denormalize_image(generated_images_256[i].cpu()).permute(1, 2, 0)\n",
    "            axes[0, i].imshow(img_256)\n",
    "            axes[0, i].set_title(f\"256x256 - Sample {i+1}\")\n",
    "            axes[0, i].axis('off')\n",
    "\n",
    "            # 64x64 images\n",
    "            img_64 = denormalize_image(generated_images_64[i].cpu()).permute(1, 2, 0)\n",
    "            axes[1, i].imshow(img_64)\n",
    "            axes[1, i].set_title(f\"64x64 - Sample {i+1}\")\n",
    "            axes[1, i].axis('off')\n",
    "\n",
    "        plt.suptitle(f'Generated Pokemon: \"{description}\"', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    generator.train()\n",
    "\n",
    "# Test with custom descriptions using enhanced functions\n",
    "test_descriptions = [\n",
    "    \"A fire type pokemon with orange fur and a flame on its tail\",\n",
    "    \"A blue water type pokemon with bubbles\",\n",
    "    \"A grass type pokemon with green leaves and vines\",\n",
    "    \"An electric type pokemon with yellow fur and lightning bolts\",\n",
    "    \"A psychic type pokemon with purple coloring and mystical powers\"\n",
    "]\n",
    "\n",
    "print(\"🔥 ENHANCED POKEMON GENERATION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing both simple and enhanced generation functions...\\n\")\n",
    "\n",
    "# Test first description with enhanced visualization\n",
    "print(f\"✨ ENHANCED Generation: {test_descriptions[0]}\")\n",
    "generate_pokemon_from_text(test_descriptions[0], num_samples=3, show_attention=False)\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Test second description with simple visualization for comparison\n",
    "print(f\"⚡ SIMPLE Generation: {test_descriptions[1]}\")\n",
    "simple_generate_pokemon(test_descriptions[1], num_samples=3)\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Quick test of remaining descriptions with simple function\n",
    "print(\"🎮 Quick tests with simple generation:\")\n",
    "for desc in test_descriptions[2:]:\n",
    "    print(f\"\\nDescription: {desc}\")\n",
    "    simple_generate_pokemon(desc, num_samples=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"💡 PRO TIP: Use show_attention=True for detailed attention analysis!\")\n",
    "print(\"Example: generate_pokemon_from_text('legendary dragon', show_attention=True)\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.5. Enhanced Visualization Demo with Utils.py Functions\n",
    "\n",
    "# Test the enhanced visualization functions from utils.py\n",
    "print(\"🎨 ENHANCED VISUALIZATION DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Test enhanced comparison grid with training data\n",
    "print(\"\\n1. Enhanced Comparison Grid:\")\n",
    "save_comparison_grid(\n",
    "    epoch=0,\n",
    "    model=generator,\n",
    "    batch=fixed_train_batch,\n",
    "    set_name=\"demo\",\n",
    "    device=device,\n",
    "    output_dir=\"demo_output\",\n",
    "    show_inline=True\n",
    ")\n",
    "\n",
    "# 2. Test attention visualization with a single sample\n",
    "print(\"\\n2. Attention Visualization (if attention is available):\")\n",
    "try:\n",
    "    save_attention_visualization(\n",
    "        epoch=0,\n",
    "        model=generator,\n",
    "        tokenizer=tokenizer,\n",
    "        batch=fixed_train_attention_batch,\n",
    "        device=device,\n",
    "        set_name=\"demo\",\n",
    "        output_dir=\"demo_output\",\n",
    "        show_inline=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Attention visualization not available: {e}\")\n",
    "    print(\"This is normal if the model doesn't have attention mechanisms enabled.\")\n",
    "\n",
    "# 3. Test enhanced loss plotting\n",
    "print(\"\\n3. Enhanced Loss Plotting:\")\n",
    "# Create some dummy loss data for demonstration\n",
    "demo_losses_g = [3.2, 2.8, 2.5, 2.2, 2.0, 1.8, 1.6, 1.5, 1.4, 1.3]\n",
    "demo_losses_d = [0.8, 0.7, 0.6, 0.65, 0.7, 0.68, 0.66, 0.64, 0.63, 0.62]\n",
    "demo_losses_recon = [0.4, 0.35, 0.3, 0.28, 0.25, 0.23, 0.22, 0.21, 0.20, 0.19]\n",
    "\n",
    "save_plot_losses(\n",
    "    losses_g=demo_losses_g,\n",
    "    losses_d=demo_losses_d,\n",
    "    losses_recon=demo_losses_recon,\n",
    "    output_dir=\"demo_output\",\n",
    "    show_inline=True\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Enhanced visualization functions successfully integrated!\")\n",
    "print(\"📁 All visualizations are saved to 'demo_output' and 'custom_generation' directories\")\n",
    "print(\"\\n🎯 Available enhanced functions:\")\n",
    "print(\"  • save_comparison_grid() - Enhanced real vs generated comparison\")\n",
    "print(\"  • save_attention_visualization() - Detailed attention heatmaps\")\n",
    "print(\"  • save_plot_losses() - Professional loss plotting\")\n",
    "print(\"  • denormalize_image() - Proper image denormalization\")\n",
    "print(\"  • generate_pokemon_from_text() - Now with attention visualization!\")\n",
    "print(\"\\n💡 Usage examples:\")\n",
    "print(\"  generate_pokemon_from_text('fire dragon', num_samples=4, show_attention=True)\")\n",
    "print(\"  simple_generate_pokemon('electric mouse', num_samples=3)  # For quick testing\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Model Analysis and Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model summary and analysis\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIKAPIKAGEN: FINAL MODEL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\\\n📊 MODEL STATISTICS:\")\n",
    "print(f\"Generator parameters: {count_parameters(generator):,}\")\n",
    "print(f\"Discriminator parameters: {count_parameters(discriminator_256):,}\")\n",
    "print(f\"Total parameters: {count_parameters(generator) + count_parameters(discriminator_256):,}\")\n",
    "\n",
    "print(f\"\\\\n📈 TRAINING STATISTICS:\")\n",
    "print(f\"Total epochs trained: {len(losses['generator'])}\")\n",
    "print(f\"Final Generator Loss: {losses['generator'][-1]:.4f}\")\n",
    "print(f\"Final Discriminator Loss: {losses['discriminator'][-1]:.4f}\")\n",
    "print(f\"Final Reconstruction Loss: {losses['reconstruction'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\\\n🎯 MODEL CAPABILITIES:\")\n",
    "print(\"✅ Text-to-Image Generation with Attention\")\n",
    "print(\"✅ BERT-mini Text Encoding (Fine-tuned)\")\n",
    "print(\"✅ Adversarial Training with Reconstruction Loss\")\n",
    "print(\"✅ Interactive Custom Text Generation\")\n",
    "print(\"✅ Real-time Training Visualization\")\n",
    "\n",
    "print(f\"\\\\n📝 ARCHITECTURE SUMMARY:\")\n",
    "print(\"• Text Encoder: Transformer-based with pre-trained BERT-mini embeddings\")\n",
    "print(\"• Generator: CNN decoder with multi-layer attention mechanism\")\n",
    "print(\"• Discriminator: CNN discriminator with text conditioning\")\n",
    "print(\"• Attention: Allows selective focus on text features during generation\")\n",
    "print(\"• Loss: Adversarial + Reconstruction (MSE) loss combination\")\n",
    "\n",
    "print(f\"\\\\n🔥 SUCCESS METRICS:\")\n",
    "print(\"• Successfully generates Pokemon sprites from text descriptions\")\n",
    "print(\"• Attention mechanism enables fine-grained text-image alignment\")\n",
    "print(\"• BERT-mini fine-tuning improves domain-specific understanding\")\n",
    "print(\"• Combined loss function balances realism and text fidelity\")\n",
    "print(\"• Real-time visualization shows training progress\")\n",
    "\n",
    "print(\"\\\\n✨ The PikaPikaGen model is now ready for Pokemon sprite generation!\")\n",
    "print(\"🎮 Try generating your own Pokemon with custom descriptions!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show final generation with interactive input\n",
    "print(\"\\\\n🎯 INTERACTIVE DEMO:\")\n",
    "print(\"Try this: generate_pokemon_from_text('Your custom Pokemon description here!')\")\n",
    "print(\"\\\\nExample: generate_pokemon_from_text('A dragon type pokemon with silver wings and red eyes', num_samples=4)\")\n",
    "\n",
    "# Quick demonstration\n",
    "generate_pokemon_from_text(\"A legendary fire dragon pokemon with golden scales\", num_samples=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
